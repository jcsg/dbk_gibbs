{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict as dd\n",
    "import copy\n",
    "import unicodecsv as csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    \"\"\"\n",
    "    Simple getter-and-setter class for storing values of hyperparameters\n",
    "    note: p_S = p_{\\$} while p_s is really that.\n",
    "    \"\"\"\n",
    "    def __init__(self,p_S,b,p_phi,alpha,p_s):\n",
    "        self.p_S = p_S\n",
    "        self.b = b\n",
    "        self.p_phi = p_phi\n",
    "        self.alpha = alpha\n",
    "        self.p_s = p_s\n",
    "        \n",
    "    def get_p_S(self):\n",
    "        return self.p_S\n",
    "    def get_b(self):\n",
    "        return self.b\n",
    "    def get_p_phi(self):\n",
    "        return self.p_phi\n",
    "    def get_alpha(self):\n",
    "        return self.alpha\n",
    "    def get_p_s(self):\n",
    "        return self.p_s\n",
    "\n",
    "    def set_p_S(self,new):\n",
    "        self.p_S = new\n",
    "    def set_b(self,new):\n",
    "        self.b = new\n",
    "    def set_p_phi(self,new):\n",
    "        self.p_phi = new\n",
    "    def set_alpha(self,new):\n",
    "        self.alpha = new\n",
    "    def set_p_s(self,new):\n",
    "        self.p_s = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To use later to improve code - make each sentence pair an object, with easy ways to modify\n",
    "# phrases and alignments\n",
    "class SentencePair:\n",
    "    def __init__(self,ind,e_sent,f_sent,phrase_pair_set):\n",
    "        def build_ph_align_objs(phrase_pair_set):\n",
    "            \"\"\"\n",
    "            Assumes pairs in the form of tuples of (e_phrase, f_phrase) = \n",
    "            ( ((0,\"Michael\"),(1,\"assumes\")), ((0,\"Michael\"),(1,\"geht\"),(1,\"davon\"),(1,\"aus\")) )\n",
    "            Note that one (or both) sides of the pair could be None, signifying\n",
    "            an unaligned phrase. No None entries are added to the list of phrases.\n",
    "            \"\"\"\n",
    "            phrases = dd(set)\n",
    "            alignment = set()\n",
    "            aligned = dd(set)\n",
    "            bounds = dd(set)\n",
    "            for pair in phrase_pair_set:\n",
    "                if pair[0]:\n",
    "                    phrases[\"e\"].add(pair[0])\n",
    "                    bounds[\"e\"].add(pair[0][0][0])\n",
    "                if pair[1]:\n",
    "                    phrases[\"f\"].add(pair[1])\n",
    "                    bounds[\"f\"].add(pair[1][0][0])\n",
    "                if pair[0] and pair[1]:\n",
    "                    alignment.add((pair[0],pair[1]))\n",
    "                    for (ind, word) in pair[0]:\n",
    "                        aligned[\"e\"].add(ind)\n",
    "                    for (ind, word) in pair[1]:\n",
    "                        aligned[\"f\"].add(ind)\n",
    "            #print(aligned[\"f\"])\n",
    "            return phrases, alignment, aligned, bounds\n",
    "\n",
    "        def build_phrase_map(self):\n",
    "            phrase_map = {\"e\":dict(), \"f\":dict()}\n",
    "            for lg in set([\"e\",\"f\"]):\n",
    "                for phrase in self.phrases[lg]:\n",
    "                    for (ind, word) in phrase:\n",
    "                        phrase_map[lg][ind] = phrase\n",
    "            return phrase_map\n",
    "        \n",
    "        def build_alignment_dict(self):\n",
    "            alignment_dict = {\"e-f\":dict(), \"f-e\":dict()}\n",
    "            for pair in self.alignment:\n",
    "                alignment_dict[\"e-f\"][pair[0]] = pair[1]\n",
    "                alignment_dict[\"f-e\"][pair[1]] = pair[0]\n",
    "            return alignment_dict\n",
    "        \n",
    "        self.ind = ind                # the line number of the sentence in the parallel text\n",
    "        self.e_sent = e_sent          # text of english (\"target\") sentence\n",
    "        self.f_sent = f_sent          # text of foreign (\"source\") sentence\n",
    "        self.phrase_pair_set = phrase_pair_set\n",
    "        self.phrases, self.alignment, self.aligned, self.bounds = build_ph_align_objs(phrase_pair_set)\n",
    "        self.phrase_map = build_phrase_map(self)\n",
    "        self.alignment_dict = build_alignment_dict(self)\n",
    "    \n",
    "    def build_phrase_map(self):\n",
    "        phrase_map = {\"e\":dict(), \"f\":dict()}\n",
    "        for lg in set([\"e\",\"f\"]):\n",
    "            for phrase in self.phrases[lg]:\n",
    "                for (ind, word) in phrase:\n",
    "                    phrase_map[lg][ind] = phrase\n",
    "        return phrase_map\n",
    "    \n",
    "    def get_phrase_map(self):\n",
    "        return self.phrase_map\n",
    "    \n",
    "    def update_phrase_map(self):\n",
    "        self.phrase_map = self.build_phrase_map()\n",
    "    \n",
    "    def query_phrase_map(self,lg,index):\n",
    "        return self.phrase_map[lg][index]\n",
    "    \n",
    "    def build_alignment_dict(self):\n",
    "        aligned = dd(set)\n",
    "        alignment_dict = {\"e-f\":dict(), \"f-e\":dict()}\n",
    "        for pair in self.alignment:\n",
    "            alignment_dict[\"e-f\"][pair[0]] = pair[1]\n",
    "            alignment_dict[\"f-e\"][pair[1]] = pair[0]\n",
    "            for (ind, word) in pair[0]:\n",
    "                aligned[\"e\"].add(ind)\n",
    "            for (ind, word) in pair[1]:\n",
    "                aligned[\"f\"].add(ind)\n",
    "        return alignment_dict, aligned\n",
    "    \n",
    "    def update_alignment_dict(self):\n",
    "        self.alignment_dict, self.aligned = self.build_alignment_dict()\n",
    "        \n",
    "    def query_alignment_dict(self,direction,phrase):\n",
    "        \"\"\"\n",
    "        direction must be either \"e-f\" or \"f-e\"\n",
    "        \"\"\"\n",
    "        if phrase in self.alignment_dict[direction]:\n",
    "            return self.alignment_dict[direction][phrase]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_ind(self):\n",
    "        return self.ind\n",
    "    \n",
    "    def get_e_sent(self):\n",
    "        return self.e_sent\n",
    "    \n",
    "    def get_f_sent(self):\n",
    "        return self.f_sent\n",
    "    \n",
    "    def get_e_phrases(self):\n",
    "        return self.phrases[\"e\"]\n",
    "    \n",
    "    def get_f_phrases(self):\n",
    "        return self.phrases[\"f\"]\n",
    "        \n",
    "    def get_aligned(self):\n",
    "        return self.aligned\n",
    "    \n",
    "    def get_alignments(self):\n",
    "        return self.alignment\n",
    "    \n",
    "    def remove_alignment(self,pair):\n",
    "        \"\"\"\n",
    "        pair must be a tuple of the shape (e_phrase,f_phrase)\n",
    "        \"\"\"\n",
    "        self.alignment.remove(pair)\n",
    "        self.update_alignment_dict()\n",
    "        #for (ind,word) in pair[0]:\n",
    "        #    self.aligned[\"e\"].remove(ind)\n",
    "        #for (ind,word) in pair[1]:\n",
    "        #    self.aligned[\"f\"].remove(ind)\n",
    "    \n",
    "    def add_alignment(self,pair):\n",
    "        \"\"\"\n",
    "        pair must be a tuple of the shape (e_phrase,f_phrase)\n",
    "        \"\"\"\n",
    "        self.alignment.add(pair)\n",
    "        self.update_alignment_dict()\n",
    "        #for (ind,word) in pair[0]:\n",
    "        #    self.aligned[\"e\"].add(ind)\n",
    "        #for (ind,word) in pair[1]:\n",
    "        #    self.aligned[\"f\"].add(ind)\n",
    "    \n",
    "    def query_boundary(self,lg,bound):\n",
    "        \"\"\"\n",
    "        lg should be \"e\" or \"f\"\n",
    "        \"\"\"\n",
    "        if bound in self.bounds[lg]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def remove_boundary(self,lg,bound):\n",
    "        \"\"\"\n",
    "        lg must be either the string \"e\" or \"f\" (english or foreign)\n",
    "        bound must be an integer\n",
    "        \"\"\"\n",
    "        self.bounds[lg].remove(bound)\n",
    "        first_phrase = None\n",
    "        second_phrase = None\n",
    "        for phrase in self.phrases[lg]:\n",
    "            for (ind,word) in phrase:\n",
    "                if ind == bound-1:\n",
    "                    first_phrase = phrase\n",
    "                elif ind == bound:\n",
    "                    second_phrase = phrase\n",
    "                if first_phrase and second_phrase:\n",
    "                    break\n",
    "        self.phrases[lg].remove(first_phrase)\n",
    "        self.phrases[lg].remove(second_phrase)\n",
    "        new_phrase = (*first_phrase,*second_phrase)\n",
    "        self.phrases[lg].add(new_phrase)\n",
    "        self.update_phrase_map()\n",
    "        \n",
    "    def add_boundary(self,lg,bound):\n",
    "        \"\"\"\n",
    "        lg must be either the string \"e\" or \"f\" (english or foreign)\n",
    "        bound must be an integer\n",
    "        \"\"\"\n",
    "        self.bounds[lg].add(bound)\n",
    "        for phrase in self.phrases[lg]:\n",
    "            for (ind, word) in phrase:\n",
    "                if ind == bound:\n",
    "                    target_phrase = phrase\n",
    "                    break\n",
    "        first_phrase = list()\n",
    "        second_phrase = list()\n",
    "        for (ind, word) in target_phrase:\n",
    "            if ind < bound:\n",
    "                first_phrase.append((ind, word))\n",
    "            else:   # elif ind >= bound\n",
    "                second_phrase.append((ind, word))\n",
    "        self.phrases[lg].remove(target_phrase)\n",
    "        self.phrases[lg].add(tuple(first_phrase))\n",
    "        self.phrases[lg].add(tuple(second_phrase))\n",
    "        self.update_phrase_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextLevelMutableObjects:\n",
    "    \"\"\"\n",
    "    There should only be one of these objects, and we typically name it TLMO in code that follows.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab=dd(set), pp_counts=Counter(), P_WA=dd(dict), P_WA_table=dd(dict), lowest_WA_prob=1.0, sent_pair_dict=dict()):\n",
    "        self.vocab = vocab\n",
    "        self.pp_counts = pp_counts              # pp_counts = phrase pair counts\n",
    "        self.P_WA = P_WA\n",
    "        self.lowest_WA_prob = lowest_WA_prob    # initialize at highest possible probability to eventually decrease it\n",
    "        self.sent_pair_dict = sent_pair_dict\n",
    "        self.P_WA_table = P_WA_table\n",
    "    \n",
    "    def get_n_e(self):\n",
    "        return len(self.vocab[\"e\"])\n",
    "    \n",
    "    def get_n_f(self):\n",
    "        return len(self.vocab[\"f\"])\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def vocab_add(self,lg,word):\n",
    "        \"\"\"\n",
    "        lg must be \"e\" or \"f\", though this isn't enforced right now.\n",
    "        No reason to have a counterpart remove method right now...\n",
    "        \"\"\"\n",
    "        self.vocab[lg].add(word)\n",
    "        \n",
    "    def init_vocab_from_text(self,lg,text):\n",
    "        for word in text.split():\n",
    "            self.vocab[lg].add(word)\n",
    "    \n",
    "    def get_pp_counts(self):\n",
    "        return self.pp_counts\n",
    "        \n",
    "    def decrement_pp_counts(self,phrase_pair):\n",
    "        \"\"\"\n",
    "        This if-statement is a kludgy way to guard against an improper balance of incrementing\n",
    "        and decrementing phrase counts. Need to closely examine how this is done in the Gibbs \n",
    "        sampling operations!\n",
    "        \"\"\"\n",
    "        if self.pp_counts[phrase_pair] > 0:\n",
    "            self.pp_counts[phrase_pair] -= 1\n",
    "    \n",
    "    def increment_pp_counts(self,phrase_pair):\n",
    "        self.pp_counts[phrase_pair] += 1\n",
    "    \n",
    "    def get_pp_count(self,phrase_pair):\n",
    "        if phrase_pair in self.pp_counts:\n",
    "            return self.pp_counts[phrase_pair]\n",
    "        else:\n",
    "            #return None\n",
    "            return 0\n",
    "    \n",
    "    def init_pp_counts_from_pair_set(self,phrase_pair_set):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        for pair in phrase_pair_set:\n",
    "            if pair[0] and pair[1]:\n",
    "                self.pp_counts[pair] += 1\n",
    "                for (ind, word) in pair[0]:\n",
    "                    self.vocab[\"e\"].add(word)\n",
    "                for (ind, word) in pair[1]:\n",
    "                    self.vocab[\"f\"].add(word)\n",
    "    \n",
    "    def get_lowest_WA_prob(self):\n",
    "        return self.lowest_WA_prob\n",
    "    \n",
    "    def set_lowest_WA_prob(self,new_val):\n",
    "        self.lowest_WA_prob = new_val\n",
    "        \n",
    "    def get_P_WA(self,word1,word2):\n",
    "        if word1 in self.P_WA:\n",
    "            if word2 in self.P_WA[word1]:\n",
    "                return self.P_WA[word1][word2]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def set_P_WA(self,word1,word2,prob):\n",
    "        self.P_WA[word1][word2] = prob\n",
    "    \n",
    "    def get_P_WA_table(self,phrase1,phrase2):\n",
    "        if phrase1 in self.P_WA_table:\n",
    "            if phrase2 in self.P_WA_table[phrase1]:\n",
    "                return self.P_WA_table[phrase1][phrase2]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def set_P_WA_table(self,phrase1,phrase2,prob):\n",
    "        self.P_WA_table[phrase1][phrase2] = prob\n",
    "    \n",
    "    def set_P_WA(self,word1,word2,prob):\n",
    "        self.P_WA[word1][word2] = prob\n",
    "        \n",
    "    def get_sent_pair_dict(self):\n",
    "        return self.sent_pair_dict\n",
    "    \n",
    "    def get_sent_pair(self,index):\n",
    "        return self.sent_pair_dict[index]\n",
    "    \n",
    "    def set_sent_pair(self,index,SentPair):\n",
    "        \"\"\"\n",
    "        SentPair should be a formal SentPair object\n",
    "        \"\"\"\n",
    "        self.sent_pair_dict[index] = SentPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_IBM_Model_1_data(src_vocab_file,trg_vocab_file,src_trg_alignment_file,trg_src_alignment_file,TLMO):\n",
    "    \"\"\"\n",
    "    Takes the output of IBM Model 1 alignments from GIZA++ and creates a lookup\n",
    "    table for the translation probabilities of f|e and e|f.\n",
    "    Here, src = foreign (f), trg = english (e)\n",
    "    Returns a lookup dictionary of the form P_WA[word1][word2] = alignment probability of word1|word2\n",
    "    # Is this correct given how GIZA++ does things?\n",
    "    TLMO = TextLevelMutableObject for holding P_WA, lowest_WA_prob\n",
    "    \"\"\"\n",
    "    src_lut = {}\n",
    "    trg_lut = {}\n",
    "    with open(trg_vocab_file,\"rb\") as tvf:\n",
    "        tv = csv.reader(tvf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in tv:\n",
    "            if len(line) == 3:\n",
    "                trg_lut[int(line[0])] = line[1]\n",
    "                TLMO.vocab_add(\"e\",line[1])\n",
    "    with open(src_vocab_file,\"rb\") as svf:\n",
    "        sv = csv.reader(svf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in sv:\n",
    "            if len(line) == 3:\n",
    "                src_lut[int(line[0])] = line[1]\n",
    "                TLMO.vocab_add(\"f\",line[1])\n",
    "    with open(trg_src_alignment_file,\"rb\") as tsaf:\n",
    "        tsa = csv.reader(tsaf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in tsa:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in trg_lut and int(line[1]) in src_lut:\n",
    "                    word1 = trg_lut[int(line[0])]\n",
    "                    word2 = src_lut[int(line[1])]\n",
    "                    prob = float(line[2])\n",
    "                    TLMO.set_P_WA(word1,word2,prob)\n",
    "                    if prob < TLMO.get_lowest_WA_prob():\n",
    "                        TLMO.set_lowest_WA_prob(prob)\n",
    "    with open(src_trg_alignment_file,\"rb\") as staf:\n",
    "        sta = csv.reader(staf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in sta:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in src_lut and int(line[1]) in trg_lut:\n",
    "                    word1 = src_lut[int(line[0])]\n",
    "                    word2 = trg_lut[int(line[1])]\n",
    "                    prob = float(line[2])\n",
    "                    TLMO.set_P_WA(word1,word2,prob)\n",
    "                    if prob < TLMO.get_lowest_WA_prob():\n",
    "                        TLMO.set_lowest_WA_prob(prob)\n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_tlmo(srctext_file,trgtext_file,alignments_file,TLMO):\n",
    "    \"\"\"\n",
    "    For now, these are the input files to the Berkeley aligner, as well as \n",
    "    the output training.align file for alignments_file.\n",
    "    Assumes that the two text input files have the same number of lines, and that these all have alignment output.\n",
    "    Assume src = Foreign (f) and trg = English (e)\n",
    "    \"\"\"\n",
    "    def extract_phrases(srctext,trgtext,alignment_output_line):\n",
    "        \"\"\"\n",
    "        Helper Function:\n",
    "        Extracts phrases from the alignment output for a single sentence.\n",
    "        For unaligned phrases, creates a phrase pair where null side is None.\n",
    "        \"\"\"\n",
    "        srclen = len(srctext.split())\n",
    "        trglen = len(trgtext.split())\n",
    "        srctext_lst = srctext.split()\n",
    "        trgtext_lst = trgtext.split()\n",
    "        src_dict = {}\n",
    "        trg_dict = {}\n",
    "        for i, word in enumerate(srctext_lst):\n",
    "            src_dict[i] = word\n",
    "        for i, word in enumerate(trgtext_lst):\n",
    "            trg_dict[i] = word\n",
    "        f_aligned = set()\n",
    "        e_aligned = set()\n",
    "        line_lst = alignment_output_line.split()\n",
    "        alignment = list()\n",
    "        for pair in line_lst:\n",
    "            [f, e] = pair.split(\"-\")\n",
    "            f_aligned.add(int(f))\n",
    "            e_aligned.add(int(e))\n",
    "            alignment.append((int(f),int(e)))\n",
    "        alignment = sorted(alignment)\n",
    "        \n",
    "        phrase_set = set()\n",
    "        phrase = set()\n",
    "        prev_e = 0\n",
    "        prev_f = 0\n",
    "        for f,e in alignment:\n",
    "            if abs(e - prev_e) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            elif abs(f - prev_f) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            else:\n",
    "                phrase.add((e,f))\n",
    "            prev_e = e\n",
    "            prev_f = f\n",
    "        phrase_set.add(frozenset(phrase))\n",
    "        \n",
    "        full_phrases = []\n",
    "        for i in sorted(phrase_set):\n",
    "            if len(i) > 0:\n",
    "                phrase_e = set()\n",
    "                phrase_f = set()\n",
    "                for pair in sorted(i):\n",
    "                    phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                    phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "                phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "                full_phrases.append(phrase_pair)\n",
    "        \n",
    "        unaligned_phrases = []\n",
    "        unaligned_phrase = set()\n",
    "        for i, word in enumerate(srctext_lst):\n",
    "            if i not in f_aligned:\n",
    "                unaligned_phrase.add((i,src_dict[i]))\n",
    "            else:\n",
    "                if len(unaligned_phrase) != 0:\n",
    "                    unaligned_phrases.append([None,sorted(unaligned_phrase)])\n",
    "                    unaligned_phrase = set()\n",
    "        if unaligned_phrase != set():\n",
    "            unaligned_phrases.append([None,sorted(unaligned_phrase)])\n",
    "            unaligned_phrase = set()\n",
    "        for i, word in enumerate(trgtext_lst):\n",
    "            if i not in e_aligned:\n",
    "                unaligned_phrase.add((i,trg_dict[i]))\n",
    "            else:\n",
    "                if len(unaligned_phrase) != 0:\n",
    "                    unaligned_phrases.append([sorted(unaligned_phrase),None])\n",
    "                    unaligned_phrase = set()\n",
    "        if unaligned_phrase != set():\n",
    "            unaligned_phrases.append([sorted(unaligned_phrase),None])\n",
    "            unaligned_phrase = set()\n",
    "        \n",
    "        full_phrases.extend(unaligned_phrases)\n",
    "        \n",
    "        full_phrase_set = set()\n",
    "        for i in full_phrases:\n",
    "            new_e = tuple(i[0]) if i[0] else None\n",
    "            new_f = tuple(i[1]) if i[1] else None\n",
    "            new_phrase = (new_e, new_f)\n",
    "            full_phrase_set.add(new_phrase)\n",
    "        \n",
    "        return full_phrase_set\n",
    "        # end function extract_phrases\n",
    "    \n",
    "    # Main code within function initialize_tlmo()\n",
    "    with open(srctext_file,\"r\",encoding=\"utf-8\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\",encoding=\"utf-8\") as trgtext_f, \\\n",
    "    open(alignments_file,\"r\") as alignments_f:\n",
    "        for i, (sline,tline,aline) in enumerate(zip(srctext_f,trgtext_f,alignments_f)):\n",
    "            src = sline.strip(\" \\n\\r\")\n",
    "            trg = tline.strip(\" \\n\\r\")\n",
    "            A = aline.strip(\" \\n\\r\")\n",
    "            full_phrase_set = extract_phrases(src,trg,A)\n",
    "            sent_pair = SentencePair(i,trg,src,full_phrase_set)\n",
    "            TLMO.init_vocab_from_text(\"e\",trg)\n",
    "            TLMO.init_vocab_from_text(\"f\",src)\n",
    "            TLMO.init_pp_counts_from_pair_set(sent_pair.get_alignments())\n",
    "            TLMO.set_sent_pair(i,sent_pair)\n",
    "    \n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collapsed_gibbs_aligner(srctext_file,trgtext_file,TLMO,hp):\n",
    "    \"\"\"\n",
    "    Will want to repeatedly call this for N iterations.\n",
    "    \"\"\"\n",
    "    # Supporting functions:\n",
    "    def p_f(src_phrase,n_f,hp):\n",
    "        return (hp.get_p_s()*(1-hp.get_p_s())**(len(src_phrase)))*((1/n_f)**(len(src_phrase)))\n",
    "    \n",
    "    def p_e(trg_phrase,n_e,hp):\n",
    "        return (hp.get_p_s()*(1-hp.get_p_s())**(len(trg_phrase)))*((1/n_e)**(len(trg_phrase)))\n",
    "    \n",
    "    def calc_P_WA(trg_phrase, src_phrase, TLMO):\n",
    "        \"\"\"\n",
    "        p(e|f)\n",
    "        src_phrase = conditioning phrase = f\n",
    "        trg_phrase = target phrase = e\n",
    "        \"\"\"\n",
    "        pre_computed_value = TLMO.get_P_WA_table(trg_phrase, src_phrase)\n",
    "        if not pre_computed_value:\n",
    "            lowest_prob = TLMO.get_lowest_WA_prob()\n",
    "            P_WA_value = 1.0\n",
    "            for i, (ind_i,word_i) in enumerate(trg_phrase):\n",
    "                i_prob = 1.0\n",
    "                for j, (ind_j,word_j) in enumerate(src_phrase):\n",
    "                    p_i_given_j = TLMO.get_P_WA(word_j,word_i)\n",
    "                    if p_i_given_j:\n",
    "                        i_prob *= p_i_given_j\n",
    "                    else:\n",
    "                        i_prob = lowest_prob\n",
    "                P_WA_value *= i_prob\n",
    "            TLMO.set_P_WA_table(trg_phrase, src_phrase, P_WA_value)\n",
    "            return P_WA_value\n",
    "        else:\n",
    "            return pre_computed_value\n",
    "    \n",
    "    def delta(trg_phrase,src_phrase,tline,sline,hp):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        s = float(len(tline))/float(len(sline))\n",
    "        delta = hp.get_b()**abs(trg_phrase[0][0]-(src_phrase[0][0]*s))\n",
    "        return delta\n",
    "    \n",
    "    def tau(trg_phrase,src_phrase,tline,sline,sent_pair,TLMO,hp):\n",
    "        tau_value = (TLMO.get_pp_count((trg_phrase,src_phrase)) + \\\n",
    "                     (hp.get_alpha() * ((p_f(src_phrase,TLMO.get_n_f(),hp) * \\\n",
    "                              calc_P_WA(trg_phrase, src_phrase, TLMO)) * \\\n",
    "                             (p_e(trg_phrase,TLMO.get_n_e(),hp) * \\\n",
    "                             calc_P_WA(src_phrase, trg_phrase, TLMO))))) / \\\n",
    "        (len(TLMO.get_pp_counts()) + hp.get_alpha())\n",
    "        return tau_value\n",
    "    \n",
    "    def theta_N(trg_phrase,src_phrase,TLMO,hp):\n",
    "        if trg_phrase and not src_phrase:\n",
    "            theta_N_value = 0.5*p_e(trg_phrase,TLMO.get_n_e(),hp)\n",
    "        elif src_phrase and not trg_phrase:\n",
    "            theta_N_value = 0.5*p_f(src_phrase,TLMO.get_n_f(),hp)\n",
    "        return theta_N_value\n",
    "        \n",
    "    def potential(trg_phrase,src_phrase,tline,sline,TLMO,sent_pair,hp):\n",
    "        \"\"\"\n",
    "        This calculates the potential function's value.\n",
    "        Watch out! Need to be able to modify to accommodate Markov blanket...\n",
    "        \"\"\"\n",
    "        if (trg_phrase and src_phrase):\n",
    "            pot_value = (1-hp.get_p_S())*(1-hp.get_p_phi())*tau(trg_phrase,src_phrase,tline,sline,sent_pair,TLMO,hp)*delta(trg_phrase,src_phrase,tline,sline,hp)\n",
    "        else:\n",
    "            pot_value = (1-hp.get_p_S())*hp.get_p_phi()*theta_N(trg_phrase,src_phrase,TLMO,hp)\n",
    "    \n",
    "        return pot_value\n",
    "    \n",
    "    #def create_phrase_map(sline,tline,full_phrases):\n",
    "    #    \"\"\"\n",
    "    #    full_phrases is the value of a key in the dictionary phrase_table, e.g. phrase_tables[0]\n",
    "    #    to get the full_phrases of the first sentence.\n",
    "    #    \"\"\"\n",
    "    #    src_phrase_map = {}\n",
    "    #    trg_phrase_map = {}\n",
    "    #    for phrase_ind, pair in enumerate(full_phrases):\n",
    "    #        trg = pair[0]\n",
    "    #        src = pair[1]\n",
    "    #        if trg:\n",
    "    #            for entry in trg:\n",
    "    #                trg_phrase_map[entry[0]] = trg\n",
    "    #        if src:\n",
    "    #            for entry in src:\n",
    "    #                src_phrase_map[entry[0]] = src\n",
    "    #    for src_ind, word in enumerate(sline):\n",
    "    #        if src_ind not in src_phrase_map:\n",
    "    #            src_phrase_map[src_ind] = None\n",
    "    #    for trg_ind, word in enumerate(tline):\n",
    "    #        if trg_ind not in trg_phrase_map:\n",
    "    #            trg_phrase_map[trg_ind] = None\n",
    "    #    return (src_phrase_map, trg_phrase_map)\n",
    "    \n",
    "    def SWAP(sent_ind,e_phrase,f_phrase,e_phrase_2,f_phrase_2,tline,sline,sent_pair,TLMO,hp):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize scorekeeping dict\n",
    "        pair_score = dict()\n",
    "        # Set up configurations as tuples. Names of variables will be keys in dict.\n",
    "        ident = (e_phrase,f_phrase,e_phrase_2,f_phrase_2)\n",
    "        swapped = (e_phrase,f_phrase_2,e_phrase_2,f_phrase)\n",
    "        out_conf_dict = {\"ident\":ident,\"swapped\":swapped}\n",
    "        # First, modifying counts and alignments so that TLMO and sent_pair are the Markov blanket.\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase))\n",
    "        sent_pair.remove_alignment((e_phrase,f_phrase))\n",
    "        sent_pair.remove_alignment((e_phrase_2,f_phrase_2))\n",
    "        # Do I need to decrement the counts for the swapped configuration?\n",
    "        # Second, calculate potential scores.\n",
    "        #     First, swapped configuration: increment phrase counts for it, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        TLMO.increment_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.increment_pp_counts((e_phrase_2,f_phrase))\n",
    "        pair_score[\"swapped\"] = (potential(e_phrase,f_phrase_2,tline,sline,TLMO,sent_pair,hp),\n",
    "                                 potential(e_phrase_2,f_phrase,tline,sline,TLMO,sent_pair,hp))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase))\n",
    "        #     Second, identify configuration: increment phrase_counts for identity, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        TLMO.increment_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.increment_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        pair_score[\"ident\"] = (potential(e_phrase,f_phrase,tline,sline,TLMO,sent_pair,hp),\n",
    "                               potential(e_phrase_2,f_phrase_2,tline,sline,TLMO,sent_pair,hp))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        # calculate output configuration probabilities\n",
    "        ident_prob = (pair_score[\"ident\"][0]*pair_score[\"ident\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        swapped_prob = (pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        #print(ident_prob+swapped_prob,ident_prob,swapped_prob)\n",
    "        # choose one configuration probabilistically\n",
    "        out_conf_decision = np.random.choice([\"ident\",\"swapped\"],1,p=[ident_prob,swapped_prob])[0]\n",
    "        out_conf = out_conf_dict[out_conf_decision]\n",
    "        # update phrase_counts, phrase_table depending on out_conf, which is a tuple of (f,e,f,e) phrases\n",
    "        TLMO.increment_pp_counts((out_conf[0],out_conf[1]))\n",
    "        TLMO.increment_pp_counts((out_conf[2],out_conf[3]))\n",
    "        # update alignments\n",
    "        sent_pair.add_alignment((out_conf[0],out_conf[1]))\n",
    "        sent_pair.add_alignment((out_conf[2],out_conf[3]))\n",
    "        sent_pair.update_alignment_dict()\n",
    "        TLMO.set_sent_pair(sent_ind,sent_pair)\n",
    "        return (sent_pair, TLMO)\n",
    "    \n",
    "    def FLIP(lg,seg_ind,e_line,f_line,sent_pair,TLMO,hp):\n",
    "        \"\"\"\n",
    "        There's some sort of bug with this where it's always assigning too high a probability to the \"is_not_a_bound\"\n",
    "        condition regardless of where you start, and the resulting action takes out all the alignments!\n",
    "        Bug both with probability mass assignment and with the behavior it causes...\n",
    "        \"\"\"\n",
    "        pair_score = {}\n",
    "        if lg == \"e\":\n",
    "            if sent_pair.query_boundary(lg,seg_ind):    # If there's a boundary, and ...\n",
    "                e_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                e_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                f_phrase_1 = sent_pair.query_alignment_dict(\"e-f\",e_phrase_1)\n",
    "                f_phrase_2 = sent_pair.query_alignment_dict(\"e-f\",e_phrase_2)\n",
    "                if f_phrase_1 and f_phrase_2:    # If both phrases are aligned:\n",
    "                    #print(\"lg:e, Both aligned!\")\n",
    "                    return (sent_pair, TLMO)             # do nothing! Pass completely to preserve Gibbs integrity.\n",
    "                pair_score[\"is_a_bound\"] = potential(e_phrase_1,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                sent_pair.remove_boundary(lg,seg_ind)    # Then, take scores for not having a boundary... update sent_pair.phrase_map\n",
    "                new_e_phrase = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                if f_phrase_2:                   # If the first phrase is aligned, score new phrase aligned to it\n",
    "                    sent_pair.remove_alignment((e_phrase_2,f_phrase_2))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "                    sent_pair.add_alignment((new_e_phrase,f_phrase_2))\n",
    "                    TLMO.increment_pp_counts((new_e_phrase,f_phrase_2))\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(new_e_phrase,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((new_e_phrase,f_phrase_2))\n",
    "                    TLMO.decrement_pp_counts((new_e_phrase,f_phrase_2))\n",
    "                elif f_phrase_1:\n",
    "                    sent_pair.remove_alignment((e_phrase_1,f_phrase_1))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_1,f_phrase_1))\n",
    "                    sent_pair.add_alignment((new_e_phrase,f_phrase_1))\n",
    "                    TLMO.increment_pp_counts((new_e_phrase,f_phrase_1))\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(new_e_phrase,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((new_e_phrase,f_phrase_1))\n",
    "                    TLMO.decrement_pp_counts((new_e_phrase,f_phrase_1))\n",
    "                else:    # if e_phrase is not aligned to either f_phrase\n",
    "                    #print(\"lg e: triggered elsewhere case\")\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(new_e_phrase,None,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                bound_prob = pair_score[\"is_a_bound\"]/(pair_score[\"is_a_bound\"]+pair_score[\"is_not_a_bound\"])\n",
    "                no_bound_prob = pair_score[\"is_not_a_bound\"]/(pair_score[\"is_a_bound\"]+pair_score[\"is_not_a_bound\"])\n",
    "                #print(\"lg: e, start:bound\", no_bound_prob+bound_prob,no_bound_prob,bound_prob)\n",
    "                out_conf_decision = np.random.choice([\"is_a_bound\",\"is_not_a_bound\"],1,p=[bound_prob,no_bound_prob])[0]\n",
    "                if out_conf_decision == \"is_a_bound\":\n",
    "                    # restore boundary\n",
    "                    if f_phrase_2:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_2,f_phrase_2))\n",
    "                        TLMO.increment_pp_counts((e_phrase_2,f_phrase_2))\n",
    "                    elif f_phrase_1:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_1,f_phrase_1))\n",
    "                        TLMO.increment_pp_counts((e_phrase_1,f_phrase_1))\n",
    "                    else:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                    return (sent_pair, TLMO)\n",
    "                elif out_conf_decision == \"is_not_a_bound\":\n",
    "                    # keep the boundary removed\n",
    "                    if f_phrase_2:\n",
    "                        sent_pair.add_alignment((new_e_phrase,f_phrase_2))\n",
    "                        TLMO.increment_pp_counts((new_e_phrase,f_phrase_2))\n",
    "                    elif f_phrase_1:\n",
    "                        sent_pair.add_alignment((new_e_phrase,f_phrase_1))\n",
    "                        TLMO.increment_pp_counts((new_e_phrase,f_phrase_1))                    \n",
    "                    return (sent_pair, TLMO)\n",
    "            else:    # If the position is not a phrase boundary, and lg is \"e\":\n",
    "                # FILL IN!!! WORK HERE!\n",
    "                e_phrase = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                f_phrase = sent_pair.query_alignment_dict(\"e-f\",e_phrase)\n",
    "                pair_score[\"is_not_a_bound\"] = potential(e_phrase,f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                if f_phrase:\n",
    "                    sent_pair.remove_alignment((e_phrase,f_phrase))\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    e_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                    e_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                    sent_pair.add_alignment((e_phrase_1,f_phrase))\n",
    "                    TLMO.increment_pp_counts((e_phrase_1,f_phrase))\n",
    "                    pair_score[\"is_a_bound_1\"] = potential(e_phrase_1,f_phrase,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,None,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase_1,f_phrase))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_1,f_phrase))\n",
    "                    sent_pair.add_alignment((e_phrase_2,f_phrase))\n",
    "                    TLMO.increment_pp_counts((e_phrase_2,f_phrase))\n",
    "                    pair_score[\"is_a_bound_2\"] = potential(e_phrase_1,None,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase_2,f_phrase))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_2,f_phrase))\n",
    "                    sent_pair.remove_boundary(lg,seg_ind)\n",
    "                else:\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    e_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                    e_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                    pair_score[\"is_a_bound_1\"] = potential(e_phrase_1,f_phrase,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,None,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    pair_score[\"is_a_bound_2\"] = potential(e_phrase_1,None,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_boundary(lg,seg_ind)\n",
    "                bound_1_prob = pair_score[\"is_a_bound_1\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                bound_2_prob = pair_score[\"is_a_bound_2\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                no_bound_prob = pair_score[\"is_not_a_bound\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                #print(\"lg: e, start:no_bound\", no_bound_prob+bound_1_prob+bound_2_prob,no_bound_prob,bound_1_prob,bound_2_prob)\n",
    "                out_conf_decision = np.random.choice([\"is_a_bound_1\",\"is_a_bound_2\",\"is_not_a_bound\"],1,p=[bound_1_prob,bound_2_prob,no_bound_prob])[0]\n",
    "                if out_conf_decision == \"is_a_bound_1\":\n",
    "                    # Need to control for whether f_phrase is aligned or not?\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    if f_phrase:\n",
    "                        e_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                        e_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_1,f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase_1,f_phrase))\n",
    "                elif out_conf_decision == \"is_a_bound_2\":\n",
    "                    # Need to control for whether f_phrase is aligned or not?\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    if f_phrase:\n",
    "                        e_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                        e_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_2,f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase_2,f_phrase))\n",
    "                elif out_conf_decision == \"is_not_a_bound\":\n",
    "                    # leave the state as no boundary\n",
    "                    if f_phrase:\n",
    "                        sent_pair.add_alignment((e_phrase,f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase,f_phrase))\n",
    "                return (sent_pair, TLMO)\n",
    "        elif lg == \"f\":\n",
    "            if sent_pair.query_boundary(lg,seg_ind):\n",
    "                f_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                f_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                e_phrase_1 = sent_pair.query_alignment_dict(\"f-e\",f_phrase_1)\n",
    "                e_phrase_2 = sent_pair.query_alignment_dict(\"f-e\",f_phrase_2)\n",
    "                if e_phrase_1 and e_phrase_2:    # If both phrases are aligned:\n",
    "                    #print(\"lg:f, Both aligned!\")\n",
    "                    return (sent_pair, TLMO)             # do nothing! Pass completely to preserve Gibbs integrity.\n",
    "                pair_score[\"is_a_bound\"] = potential(e_phrase_1,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase_2,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                sent_pair.remove_boundary(lg,seg_ind)    # Then, take scores for not having a boundary... update sent_pair.phrase_map\n",
    "                new_f_phrase = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                if e_phrase_2:                   # If the first phrase is aligned, score new phrase aligned to it\n",
    "                    sent_pair.remove_alignment((e_phrase_2,f_phrase_2))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "                    sent_pair.add_alignment((e_phrase_2,new_f_phrase))\n",
    "                    TLMO.increment_pp_counts((e_phrase_2,new_f_phrase))\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(e_phrase_2,new_f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase_2,new_f_phrase))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_2,new_f_phrase))\n",
    "                elif e_phrase_1:\n",
    "                    sent_pair.remove_alignment((e_phrase_1,f_phrase_1))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_1,f_phrase_1))\n",
    "                    sent_pair.add_alignment((e_phrase_1,new_f_phrase))\n",
    "                    TLMO.increment_pp_counts((e_phrase_1,new_f_phrase))\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(e_phrase_1,new_f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase_1,new_f_phrase))\n",
    "                    TLMO.decrement_pp_counts((e_phrase_1,new_f_phrase))\n",
    "                else:   # if neither f_phrase is aligned\n",
    "                    pair_score[\"is_not_a_bound\"] = potential(None,new_f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                bound_prob = pair_score[\"is_a_bound\"]/(pair_score[\"is_a_bound\"]+pair_score[\"is_not_a_bound\"])\n",
    "                no_bound_prob = pair_score[\"is_not_a_bound\"]/(pair_score[\"is_a_bound\"]+pair_score[\"is_not_a_bound\"])\n",
    "                #print(\"lg: f, start:bound\", no_bound_prob+bound_prob,no_bound_prob,bound_prob)\n",
    "                out_conf_decision = np.random.choice([\"is_a_bound\",\"is_not_a_bound\"],1,p=[bound_prob,no_bound_prob])[0]\n",
    "                if out_conf_decision == \"is_a_bound\":\n",
    "                    # restore boundary\n",
    "                    if e_phrase_2:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_2,f_phrase_2))\n",
    "                        TLMO.increment_pp_counts((e_phrase_2,f_phrase_2))\n",
    "                    elif e_phrase_1:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase_1,f_phrase_1))\n",
    "                        TLMO.increment_pp_counts((e_phrase_1,f_phrase_1))\n",
    "                    else:\n",
    "                        sent_pair.add_boundary(lg,seg_ind)\n",
    "                    return (sent_pair, TLMO)\n",
    "                elif out_conf_decision == \"is_not_a_bound\":\n",
    "                    # keep the boundary removed\n",
    "                    if e_phrase_2:\n",
    "                        sent_pair.add_alignment((e_phrase_2,new_f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase_2,new_f_phrase))\n",
    "                    elif e_phrase_1:\n",
    "                        sent_pair.add_alignment((e_phrase_1,new_f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase_1,new_f_phrase))\n",
    "                    return (sent_pair, TLMO)\n",
    "            else:    # If the position is not a phrase boundary, and lg is \"f\":\n",
    "                f_phrase = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                e_phrase = sent_pair.query_alignment_dict(\"f-e\",f_phrase)\n",
    "                pair_score[\"is_not_a_bound\"] = potential(e_phrase,f_phrase,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                if e_phrase:\n",
    "                    sent_pair.remove_alignment((e_phrase,f_phrase))\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    f_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                    f_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                    sent_pair.add_alignment((e_phrase,f_phrase_1))\n",
    "                    TLMO.increment_pp_counts((e_phrase,f_phrase_1))\n",
    "                    pair_score[\"is_a_bound_1\"] = potential(e_phrase,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(None,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase,f_phrase_1))\n",
    "                    TLMO.decrement_pp_counts((e_phrase,f_phrase_1))\n",
    "                    sent_pair.add_alignment((e_phrase,f_phrase_2))\n",
    "                    TLMO.increment_pp_counts((e_phrase,f_phrase_2))\n",
    "                    pair_score[\"is_a_bound_2\"] = potential(None,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_alignment((e_phrase,f_phrase_2))\n",
    "                    TLMO.decrement_pp_counts((e_phrase,f_phrase_2))\n",
    "                    sent_pair.remove_boundary(lg,seg_ind)\n",
    "                else:\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    f_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                    f_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                    pair_score[\"is_a_bound_1\"] = potential(e_phrase,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(None,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    pair_score[\"is_a_bound_2\"] = potential(None,f_phrase_1,e_line,f_line,TLMO,sent_pair,hp)*potential(e_phrase,f_phrase_2,e_line,f_line,TLMO,sent_pair,hp)\n",
    "                    sent_pair.remove_boundary(lg,seg_ind)\n",
    "                bound_1_prob = pair_score[\"is_a_bound_1\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                bound_2_prob = pair_score[\"is_a_bound_2\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                no_bound_prob = pair_score[\"is_not_a_bound\"]/(pair_score[\"is_a_bound_1\"]+pair_score[\"is_a_bound_2\"]+pair_score[\"is_not_a_bound\"])\n",
    "                #print(\"lg: f, start:no_bound\", no_bound_prob+bound_1_prob+bound_2_prob,no_bound_prob,bound_1_prob,bound_2_prob)\n",
    "                out_conf_decision = np.random.choice([\"is_a_bound_1\",\"is_a_bound_2\",\"is_not_a_bound\"],1,p=[bound_1_prob,bound_2_prob,no_bound_prob])[0]\n",
    "                if out_conf_decision == \"is_a_bound_1\":\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    if e_phrase:\n",
    "                        f_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                        f_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase,f_phrase_1))\n",
    "                        TLMO.increment_pp_counts((e_phrase,f_phrase_1))\n",
    "                    return (sent_pair, TLMO)\n",
    "                elif out_conf_decision == \"is_a_bound_2\":\n",
    "                    sent_pair.add_boundary(lg,seg_ind)\n",
    "                    if e_phrase:\n",
    "                        f_phrase_1 = sent_pair.query_phrase_map(lg,seg_ind-1)\n",
    "                        f_phrase_2 = sent_pair.query_phrase_map(lg,seg_ind)\n",
    "                        sent_pair.add_alignment((e_phrase,f_phrase_2))\n",
    "                        TLMO.increment_pp_counts((e_phrase,f_phrase_2))\n",
    "                    return (sent_pair, TLMO)\n",
    "                elif out_conf_decision == \"is_not_a_bound\":\n",
    "                    # leave the state as no boundary\n",
    "                    if e_phrase:\n",
    "                        sent_pair.add_alignment((e_phrase,f_phrase))\n",
    "                        TLMO.increment_pp_counts((e_phrase,f_phrase))\n",
    "                    return (sent_pair, TLMO)\n",
    "    \n",
    "    def TOGGLE(trg_ph,src_ph,e_line,f_line,sent_pair,TLMO,hp):\n",
    "        \"\"\"\n",
    "        There seems to be a bug here where TOGGLE consistently lowers the number of aligned phrase pairs, if allowed\n",
    "        to run enough time. \n",
    "        \"\"\"\n",
    "        pair_score = dict()\n",
    "        # create Markov blanket to start\n",
    "        if (trg_ph,src_ph) in sent_pair.get_alignments():\n",
    "            TLMO.decrement_pp_counts((trg_ph,src_ph))\n",
    "            sent_pair.remove_alignment((trg_ph,src_ph))\n",
    "        # calculate score for no alignment\n",
    "        pair_score[\"unaligned\"] = potential(trg_ph,src_ph,e_line,f_line,TLMO,sent_pair,hp)\n",
    "        # calculate score for both phrases aligned\n",
    "        TLMO.increment_pp_counts((trg_ph,src_ph))\n",
    "        sent_pair.add_alignment((trg_ph,src_ph))\n",
    "        pair_score[\"aligned\"] = potential(trg_ph,src_ph,e_line,f_line,TLMO,sent_pair,hp)\n",
    "        TLMO.decrement_pp_counts((trg_ph,src_ph))\n",
    "        sent_pair.remove_alignment((trg_ph,src_ph))\n",
    "        # now back to Markov blanket\n",
    "        unaligned_prob = pair_score[\"unaligned\"]/(pair_score[\"unaligned\"]+pair_score[\"aligned\"])\n",
    "        aligned_prob = pair_score[\"aligned\"]/(pair_score[\"unaligned\"]+pair_score[\"aligned\"])\n",
    "        #print(unaligned_prob+aligned_prob,unaligned_prob,aligned_prob)\n",
    "        out_conf_decision = np.random.choice([\"unaligned\",\"aligned\"],1,p=[unaligned_prob,aligned_prob])[0]\n",
    "        if out_conf_decision == \"unaligned\":\n",
    "            sent_pair.update_alignment_dict()\n",
    "            return (sent_pair, TLMO)\n",
    "        elif out_conf_decision == \"aligned\":\n",
    "            sent_pair.add_alignment((trg_ph,src_ph))\n",
    "            TLMO.increment_pp_counts((trg_ph,src_ph))\n",
    "            sent_pair.update_alignment_dict()\n",
    "            return (sent_pair, TLMO)\n",
    "    \n",
    "    \n",
    "    # Main code for collapsed Gibbs sampler\n",
    "    with open(srctext_file,\"r\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\") as trgtext_f:\n",
    "        for sent_ind, (fline, eline) in enumerate(zip(srctext_f,trgtext_f)):\n",
    "            #print(sent_ind)\n",
    "            sent_pair = TLMO.get_sent_pair(sent_ind)\n",
    "            e_line = eline.strip(\" \\n\\r\").split()\n",
    "            f_line = fline.strip(\" \\n\\r\").split()\n",
    "            e_prev = None\n",
    "            for f_ind, f_word in enumerate(f_line):\n",
    "                for e_ind, e_word in enumerate(e_line):\n",
    "                    # Apply SWAP, if applicable\n",
    "                    # Not done: CHECK TO ENSURE THAT WE'RE NOT MISSING ANY PHRASES IN EITHER LANGUAGE!\n",
    "                    e_phrase = sent_pair.query_phrase_map(\"e\",e_ind)\n",
    "                    f_phrase = sent_pair.query_alignment_dict(\"e-f\",e_phrase)\n",
    "                    if f_phrase and (e_phrase != e_prev):\n",
    "                        sorted_e_ph = sorted(sent_pair.get_e_phrases())\n",
    "                        next_e_ind = (sorted_e_ph.index(e_phrase))+1\n",
    "                        if next_e_ind <= len(sorted_e_ph)-1:\n",
    "                            e_phrase_2 = sorted_e_ph[next_e_ind]\n",
    "                            f_phrase_2 = sent_pair.query_alignment_dict(\"e-f\",e_phrase_2)\n",
    "                            # actual call to SWAP.\n",
    "                            if f_phrase_2:\n",
    "                                sent_pair, TLMO = SWAP(sent_ind,e_phrase,f_phrase,e_phrase_2,f_phrase_2,e_line,f_line,sent_pair,TLMO,hp)\n",
    "                                #print(\"SWAP: \",sent_pair.get_aligned()[\"f\"])\n",
    "                    del f_phrase\n",
    "                    # Apply FLIP, if applicable\n",
    "                    if e_ind != 0:\n",
    "                        sent_pair, TLMO = FLIP(\"e\",e_ind,e_line,f_line,sent_pair,TLMO,hp)\n",
    "                    if f_ind != 0:\n",
    "                        sent_pair, TLMO = FLIP(\"f\",f_ind,e_line,f_line,sent_pair,TLMO,hp)\n",
    "                    # Apply TOGGLE, if applicable\n",
    "                    f_phrase = sent_pair.query_phrase_map(\"f\",f_ind)\n",
    "                    if f_ind != 0:\n",
    "                        f_prev = sent_pair.query_phrase_map(\"f\",f_ind-1)\n",
    "                    if e_phrase != e_prev:\n",
    "                        if ((e_phrase,f_phrase) in sent_pair.get_alignments()) or (not e_phrase and not f_phrase):\n",
    "                            sent_pair, TLMO = TOGGLE(e_phrase,f_phrase,e_line,f_line,sent_pair,TLMO,hp)\n",
    "                            #print(\"TOGGLE: \",sent_pair.get_aligned()[\"f\"])\n",
    "                    # Apply FLIP TWO, if applicable\n",
    "                    \n",
    "                    # Apply MOVE, if applicable\n",
    "                    \n",
    "                    e_prev = e_phrase\n",
    "#            final_probability = 0   # PLACEHOLDER - fill with model calculation.\n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_alignments(TLMO):\n",
    "    sent_pair_dict = TLMO.get_sent_pair_dict()\n",
    "    for i in sent_pair_dict:\n",
    "        print(str(i)+\": \")\n",
    "        alignments = sent_pair_dict[i].get_alignments()\n",
    "        for (e, f) in sorted(alignments,key=lambda x:x[0][0][0]):\n",
    "            print(\"\\t\",e,\"\\t\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"main\":\n",
    "    # Use argparse to allow user to enter hyperparameters and input files\n",
    "    hp = Hyperparameters(p_S,b,p_phi,alpha,p_s)\n",
    "    TLMO = TextLevelMutableObjects()\n",
    "    TLMO = process_IBM_Model_1_data(src_vocab_file,trg_vocab_file,src_trg_alignment_file,trg_src_alignment_file,TLMO)\n",
    "    TLMO = initialize_tlmo(srctext_file,trgtext_file,alignments_file,TLMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_vocab_file = \"../data/example/two_line/f-srctext-spa.vcb\"\n",
    "trg_vocab_file = \"../data/example/two_line/e-trgtext-eng.vcb\"\n",
    "src_trg_alignment_file = \"../data/example/two_line/f_e_word_alignment_probs.txt\"\n",
    "trg_src_alignment_file = \"../data/example/two_line/e_f_word_alignment_probs.txt\"\n",
    "srctext_file = \"../data/example/two_line/f-srctext-spa.txt\"\n",
    "trgtext_file = \"../data/example/two_line/e-trgtext-eng.txt\"\n",
    "alignments_file = \"../data/example/two_line/berkeley_alignments_spa_eng.txt\"\n",
    "\n",
    "hp = Hyperparameters(0.1,0.85,10e-10,100,0.8)\n",
    "TLMO = TextLevelMutableObjects()\n",
    "TLMO = process_IBM_Model_1_data(src_vocab_file,trg_vocab_file,src_trg_alignment_file,trg_src_alignment_file,TLMO)\n",
    "TLMO = initialize_tlmo(srctext_file,trgtext_file,alignments_file,TLMO)\n",
    "TLMO = collapsed_gibbs_aligner(srctext_file,trgtext_file,TLMO,hp)\n",
    "# Need to resolve bug in which FLIP is assigning 99%-100% probability to \"is_not_a_bound\" in almost every case! Why?\n",
    "# Need to resolve bug in which potential() is being asked (in FLIP) to evaluate a <None, None> \"phrase\" pair.\n",
    "#print_alignments(TLMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-0c6a3918cf0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "new = (((15, '.'),), None)\n",
    "print(new[0][0][0],len(new[0][0][0]),type(new[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_phrases(srctext,trgtext,alignment_output_line):\n",
    "    \"\"\"\n",
    "    Helper Function:\n",
    "    Extracts phrases from the alignment output for a single sentence.\n",
    "    For unaligned phrases, creates a phrase pair where null side is None.\n",
    "    \"\"\"\n",
    "    srclen = len(srctext.split())\n",
    "    trglen = len(trgtext.split())\n",
    "    srctext_lst = srctext.split()\n",
    "    trgtext_lst = trgtext.split()\n",
    "    src_dict = {}\n",
    "    trg_dict = {}\n",
    "    for i, word in enumerate(srctext_lst):\n",
    "        src_dict[i] = word\n",
    "    for i, word in enumerate(trgtext_lst):\n",
    "        trg_dict[i] = word\n",
    "    f_aligned = set()\n",
    "    e_aligned = set()\n",
    "    line_lst = alignment_output_line.split()\n",
    "    alignment = list()\n",
    "    for pair in line_lst:\n",
    "        [f, e] = pair.split(\"-\")\n",
    "        f_aligned.add(int(f))\n",
    "        e_aligned.add(int(e))\n",
    "        alignment.append((int(f),int(e)))\n",
    "    alignment = sorted(alignment)\n",
    "    \n",
    "    phrase_set = set()\n",
    "    phrase = set()\n",
    "    prev_e = 0\n",
    "    prev_f = 0\n",
    "    for f,e in alignment:\n",
    "        if abs(e - prev_e) > 1:\n",
    "            phrase_set.add(frozenset(phrase))\n",
    "            phrase = set([(e,f)])\n",
    "        elif abs(f - prev_f) > 1:\n",
    "            phrase_set.add(frozenset(phrase))\n",
    "            phrase = set([(e,f)])\n",
    "        else:\n",
    "            phrase.add((e,f))\n",
    "        prev_e = e\n",
    "        prev_f = f\n",
    "    phrase_set.add(frozenset(phrase))\n",
    "\n",
    "    full_phrases = []\n",
    "    for i in sorted(phrase_set):\n",
    "        if len(i) > 0:\n",
    "            phrase_e = set()\n",
    "            phrase_f = set()\n",
    "            for pair in sorted(i):\n",
    "                phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "            phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "            full_phrases.append(phrase_pair)\n",
    "    \n",
    "    unaligned_phrases = []\n",
    "    unaligned_phrase = []\n",
    "    for i, word in enumerate(srctext_lst):\n",
    "        print(i,word)\n",
    "        if i not in f_aligned:\n",
    "            unaligned_phrase.append((i,src_dict[i]))\n",
    "        else:\n",
    "            if len(unaligned_phrase) != 0:\n",
    "                print(unaligned_phrase)\n",
    "                unaligned_phrases.append((None,tuple(sorted(unaligned_phrase))))\n",
    "                unaligned_phrase = []\n",
    "    if unaligned_phrase != []:\n",
    "        unaligned_phrases.append((None,tuple(sorted(unaligned_phrase))))\n",
    "        unaligned_phrase = []\n",
    "    for i, word in enumerate(trgtext_lst):\n",
    "        print(i,word)\n",
    "        if i not in e_aligned:\n",
    "            unaligned_phrase.append((i,trg_dict[i]))\n",
    "        else:\n",
    "            if len(unaligned_phrase) != 0:\n",
    "                print(unaligned_phrase)\n",
    "                unaligned_phrases.append((tuple(sorted(unaligned_phrase)),None))\n",
    "                unaligned_phrase = []\n",
    "    if unaligned_phrase != []:\n",
    "        unaligned_phrases.append((tuple(sorted(unaligned_phrase)),None))\n",
    "        unaligned_phrase = []\n",
    "        \n",
    "    full_phrases.extend(unaligned_phrases)\n",
    "    \n",
    "    full_phrase_set = set()\n",
    "    for i in full_phrases:\n",
    "        new_e = tuple(i[0]) if i[0] else None\n",
    "        new_f = tuple(i[1]) if i[1] else None\n",
    "        new_phrase = (new_e, new_f)\n",
    "        full_phrase_set.add(new_phrase)\n",
    "\n",
    "    return full_phrase_set\n",
    "\n",
    "srctext = \"Judá fue padre de Fares y de Zérah , y su madre fue Tamar . Fares fue padre de Hesrón y éste de Aram .\"\n",
    "trgtext = \"Judah and Tamar were the father and mother of Perez and Zerah . Perez was the father of Hezron , Hezron the father of Ram ,\"\n",
    "alignment_output_line = \"0-0 17-16 7-11 16-14 5-10 13-2 15-13 4-9 14-12 3-8 13-11 23-24 22-23 11-7 9-6 19-20 19-18 18-17\"\n",
    "full_phrase_set = extract_phrases(srctext,trgtext,alignment_output_line)\n",
    "for i in full_phrase_set:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
