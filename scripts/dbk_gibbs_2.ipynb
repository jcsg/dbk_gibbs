{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict as dd\n",
    "import copy\n",
    "import unicodecsv as csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    def __init__(self,p_S,b,p_phi,alpha,p_s):\n",
    "        self.p_S = p_S\n",
    "        self.b = b\n",
    "        self.p_phi = p_phi\n",
    "        self.alpha = alpha\n",
    "        self.p_s = p_s\n",
    "        \n",
    "    def get_p_S():\n",
    "        return self.p_S\n",
    "    def get_b():\n",
    "        return self.b\n",
    "    def get_p_phi():\n",
    "        return self.p_phi\n",
    "    def get_alpha():\n",
    "        return self.alpha\n",
    "    def get_p_s():\n",
    "        return self.p_s\n",
    "\n",
    "    def set_p_S(new):\n",
    "        self.p_S = new\n",
    "    def set_b(new):\n",
    "        self.b = new\n",
    "    def set_p_phi(new):\n",
    "        self.p_phi = new\n",
    "    def set_alpha(new):\n",
    "        self.alpha = new\n",
    "    def set_p_s(new):\n",
    "        self.p_s = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use later to improve code - make each sentence pair an object, with easy ways to modify\n",
    "# phrases and alignments\n",
    "class SentencePair:\n",
    "    def __init__(self,ind,e_sent,f_sent,phrase_pair_set):\n",
    "        self.ind = ind                # the line number of the sentence in the parallel text\n",
    "        self.e_sent = e_sent          # text of english (\"target\") sentence\n",
    "        self.f_sent = f_sent          # text of foreign (\"source\") sentence\n",
    "        self.phrases, self.alignment, self.aligned = build_ph_align_objs(phrase_pair_set)\n",
    "        self.phrase_map = build_phrase_map(self,self.phrases)\n",
    "        self.alignment_dict = build_alignment_dict(self,self.alignment)\n",
    "    \n",
    "    def build_ph_align_objs(phrase_pair_set):\n",
    "        \"\"\"\n",
    "        Assumes pairs in the form of tuples of (e_phrase, f_phrase) = \n",
    "        ( ((0,\"Michael\"),(1,\"assumes\")), ((0,\"Michael\"),(1,\"geht\"),(1,\"davon\"),(1,\"aus\")) )\n",
    "        \"\"\"\n",
    "        phrases = dd(set)\n",
    "        alignment = set()\n",
    "        aligned = dd(set)\n",
    "        boundaries = dd(set)\n",
    "        for pair in phrase_pair_set:\n",
    "            if pair[0]:\n",
    "                phrases[\"e\"].add(pair[0])\n",
    "                boundaries[\"e\"].add(pair[0][0][0])\n",
    "            if pair[1]:\n",
    "                phrases[\"f\"].add(pair[1])\n",
    "                boundaries[\"f\"].add(pair[1][0][0])\n",
    "            if pair[0] and pair[1]:\n",
    "                alignment.add((pair[0],pair[1]))\n",
    "                for (ind, word) in pair[0]:\n",
    "                    aligned[\"e\"].add(ind)\n",
    "                for (ind, word) in pair[1]:\n",
    "                    aligned[\"f\"].add(ind)\n",
    "        return phrases, alignment, aligned\n",
    "    \n",
    "    def build_phrase_map(self,self.phrases):\n",
    "        phrase_map = {\"e\":dict(), \"f\":dict()}\n",
    "        for lg in set([\"e\",\"f\"]):\n",
    "            for phrase in self.phrases[lg]:\n",
    "                for (ind, word) in phrase:\n",
    "                    phrase_map[lg][ind] = phrase\n",
    "                    \n",
    "    def update_phrase_map(self):\n",
    "        self.phrase_map = build_phrase_map(self.phrases)\n",
    "        \n",
    "    def query_phrase_map(self,lg,index):\n",
    "        return self.phrase_map[lg][index]\n",
    "    \n",
    "    def build_alignment_dict(self,self.alignment):\n",
    "        alignment_dict = {\"e-f\":dict(), \"f-e\":dict()}\n",
    "        for pair in self.alignment:\n",
    "            alignment_dict[\"e-f\"][pair[0]] = pair[1]\n",
    "            alignment_dict[\"f-e\"][pair[1]] = pair[0]\n",
    "        return alignment_dict\n",
    "    \n",
    "    def update_alignment_dict(self)\n",
    "        self.alignment_dict = build_alignment_dict(self,self.alignment)\n",
    "        \n",
    "    def query_alignment_dict(self,direction,phrase):\n",
    "        \"\"\"\n",
    "        direction must be either \"e-f\" or \"f-e\"\n",
    "        \"\"\"\n",
    "        if phrase in self.alignment_dict[direction]:\n",
    "            return self.alignment_dict[direction][phrase]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_ind(self):\n",
    "        return self.ind\n",
    "    \n",
    "    def get_e_sent(self):\n",
    "        return self.e_sent\n",
    "    \n",
    "    def get_f_sent(self):\n",
    "        return self.f_sent\n",
    "    \n",
    "    def get_e_ph(self):\n",
    "        return self.e_ph\n",
    "    \n",
    "    def get_f_ph(self):\n",
    "        return self.f_ph\n",
    "    \n",
    "    def get_alignment(self):\n",
    "        return self.alignment\n",
    "    \n",
    "    def get_aligned(self):\n",
    "        return self.aligned\n",
    "    \n",
    "    def remove_alignment(self,pair):\n",
    "        \"\"\"\n",
    "        pair must be a tuple of the shape (e_phrase,f_phrase)\n",
    "        \"\"\"\n",
    "        self.alignment.remove(pair)\n",
    "        for (ind,word) in pair[0]:\n",
    "            self.aligned[\"e\"].remove(ind)\n",
    "        for (ind,word) in pair[1]:\n",
    "            self.aligned[\"f\"].remove(ind)\n",
    "    \n",
    "    def add_alignment(self,pair):\n",
    "        \"\"\"\n",
    "        pair must be a tuple of the shape (e_phrase,f_phrase)\n",
    "        \"\"\"\n",
    "        self.alignment.add(pair)\n",
    "        for (ind,word) in pair[0]:\n",
    "            self.aligned[\"e\"].add(ind)\n",
    "        for (ind,word) in pair[1]:\n",
    "            self.aligned[\"f\"].add(ind)\n",
    "            \n",
    "    def query_alignment_e(self,e_phrase)\n",
    "            \n",
    "    def remove_boundary(self,lg,bound):\n",
    "        \"\"\"\n",
    "        lg must be either the string \"e\" or \"f\" (english or foreign)\n",
    "        bound must be an integer\n",
    "        \"\"\"\n",
    "        self.boundaries[lg].remove(bound)\n",
    "        for phrase in self.phrases[lg]:\n",
    "            for (ind,word) in phrase:\n",
    "                if ind == bound-1:\n",
    "                    first_phrase = phrase\n",
    "                if ind == bound:\n",
    "                    second_phrase = phrase\n",
    "                if first_phrase and second_phrase:\n",
    "                    break\n",
    "        self.phrases[lg].remove(first_phrase)\n",
    "        self.phrases[lg].remove(second_phrase)\n",
    "        new_phrase = (*first_phrase,*second_phrase)\n",
    "        self.phrases[lg].add(new_phrase)\n",
    "        \n",
    "    def add_boundary(self,lg,bound):\n",
    "        \"\"\"\n",
    "        lg must be either the string \"e\" or \"f\" (english or foreign)\n",
    "        bound must be an integer\n",
    "        \"\"\"\n",
    "        self.boundaries[lg].add(bound)\n",
    "        for phrase in self.phrases[lg]:\n",
    "            for (ind, word) in phrase:\n",
    "                if ind == bound:\n",
    "                    target_phrase = phrase\n",
    "                    break\n",
    "        first_phrase = list()\n",
    "        second_phrase = list()\n",
    "        for (ind, word) in target_phrase:\n",
    "            if ind < bound:\n",
    "                first_phrase.append((ind, word))\n",
    "            else:   # elif ind >= bound\n",
    "                second_phrase.append((ind, word))\n",
    "        self.phrases[lg].remove(target_phrase)\n",
    "        self.phrases[lg].add(tuple(first_phrase))\n",
    "        self.phrases[lg].add(tuple(second_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextLevelMutableObjects:\n",
    "    def __init__():\n",
    "        self.vocab = dd(set)\n",
    "        self.pp_counts = Counter()      # pp_counts = phrase pair counts\n",
    "        self.P_WA = dd(dict)\n",
    "        self.lowest_WA_prob = 1.0       # initialize at highest possible probability to eventually decrease it\n",
    "        self.sent_pair_dict = dict()\n",
    "    \n",
    "    def get_n_e(self):\n",
    "        return len(self.vocab[\"e\"])\n",
    "    \n",
    "    def get_n_f(self):\n",
    "        return len(self.vocab[\"f\"])\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def vocab_add(self,lg,word):\n",
    "        \"\"\"\n",
    "        lg must be \"e\" or \"f\", though this isn't enforced right now.\n",
    "        No reason to have a counterpart remove method right now...\n",
    "        \"\"\"\n",
    "        self.vocab[lg].add(word)\n",
    "        \n",
    "    def init_vocab_from_text(self,lg,text):\n",
    "        for word in text.split():\n",
    "            self.vocab[lg].add(word)\n",
    "    \n",
    "    def get_pp_counts(self):\n",
    "        return self.pp_counts\n",
    "        \n",
    "    def decrement_pp_counts(self,phrase_pair):\n",
    "        self.pp_counts[phrase_pair] -= 1\n",
    "    \n",
    "    def increment_pp_counts(self,phrase_pair):\n",
    "        self.pp_counts[phrase_pair] += 1\n",
    "    \n",
    "    def get_pp_count(self,phrase_pair):\n",
    "        return self.pp_counts[phrase_pair]\n",
    "    \n",
    "    def init_pp_counts_from_pair_set(self,phrase_pair_set):\n",
    "        \"\"\"\n",
    "        Careful! This assumes that these are true phrase pairs, where both sides are non-null.\n",
    "        Guard against violations of this if necessary.\n",
    "        \"\"\"\n",
    "        for pair in phrase_pair_set:\n",
    "            self.pp_counts[pair] += 1\n",
    "            for (ind, word) in pair[0]:\n",
    "                self.vocab[\"e\"].add(word)\n",
    "            for (ind, word) in pair[1]:\n",
    "                self.vocab[\"f\"].add(word)\n",
    "    \n",
    "    def get_lowest_WA_prob(self):\n",
    "        return self.lowest_WA_prob\n",
    "    \n",
    "    def set_lowest_WA_prob(self,new_val):\n",
    "        self.lowest_WA_prob = new_val\n",
    "        \n",
    "    def get_P_WA(self,word1,word2):\n",
    "        return self.P_WA[word1][word2]\n",
    "    \n",
    "    def set_P_WA(self,word1,word2,prob):\n",
    "        self.P_WA[word1][word2] = prob\n",
    "        \n",
    "    def get_sent_pair_dict(self):\n",
    "        return self.sent_pair_dict\n",
    "    \n",
    "    def get_sent_pair(self,index):\n",
    "        return self.sent_pair_dict[index]\n",
    "    \n",
    "    def set_sent_pair(self,index,SentPair):\n",
    "        \"\"\"\n",
    "        SentPair should be a formal SentPair object\n",
    "        \"\"\"\n",
    "        self.sent_pair_dict[index] = SentPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_IBM_Model_1_data(src_vocab_file,trg_vocab_file,src_trg_alignment_file,trg_src_alignment_file,TLMO):\n",
    "    \"\"\"\n",
    "    Takes the output of IBM Model 1 alignments from GIZA++ and creates a lookup\n",
    "    table for the translation probabilities of f|e and e|f.\n",
    "    Here, src = foreign (f), trg = english (e)\n",
    "    Returns a lookup dictionary of the form P_WA[word1][word2] = alignment probability of word1|word2\n",
    "    # Is this correct given how GIZA++ does things?\n",
    "    TLMO = TextLevelMutableObject for holding P_WA, lowest_WA_prob\n",
    "    \"\"\"\n",
    "    src_lut = {}\n",
    "    trg_lut = {}\n",
    "    with open(trg_vocab_file,\"rb\") as tvf:\n",
    "        tv = csv.reader(tvf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in tv:\n",
    "            if len(line) == 3:\n",
    "                trg_lut[int(line[0])] = line[1]\n",
    "                TLMO.vocab_add(\"e\",line[1])\n",
    "    with open(src_vocab_file,\"rb\") as svf:\n",
    "        sv = csv.reader(svf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in sv:\n",
    "            if len(line) == 3:\n",
    "                src_lut[int(line[0])] = line[1]\n",
    "                TLMO.vocab_add(\"f\",line[1])\n",
    "    with open(trg_src_alignment_file,\"rb\") as tsaf:\n",
    "        tsa = csv.reader(tsaf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in tsa:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in trg_lut and int(line[1]) in src_lut:\n",
    "                    word1 = trg_lut[int(line[0])]\n",
    "                    word2 = src_lut[int(line[1])]\n",
    "                    prob = float(line[2])\n",
    "                    TLMO.set_P_WA[word1][word2] = prob\n",
    "                    if prob < TLMO.get_lowest_WA_prob():\n",
    "                        TLMO.set_lowest_WA_prob(prob)\n",
    "    with open(src_trg_alignment_file,\"rb\") as staf:\n",
    "        sta = csv.reader(staf,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in sta:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in src_lut and int(line[1]) in trg_lut:\n",
    "                    word1 = src_lut[int(line[0])]\n",
    "                    word2 = trg_lut[int(line[1])]\n",
    "                    prob = float(line[2])\n",
    "                    TLMO.set_P_WA[word1][word2] = prob\n",
    "                    if prob < TLMO.get_lowest_WA_prob():\n",
    "                        TLMO.set_lowest_WA_prob(prob)\n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_tlmo(srctext_file,trgtext_file,alignments_file,TLMO):\n",
    "    \"\"\"\n",
    "    For now, these are the input files to the Berkeley aligner, as well as \n",
    "    the output training.align file for alignments_file.\n",
    "    Assumes that the two text input files have the same number of lines, and that these all have alignment output.\n",
    "    Assume src = Foreign (f) and trg = English (e)\n",
    "    \"\"\"\n",
    "    def extract_phrases(srctext,trgtext,alignment_output_line):\n",
    "        \"\"\"\n",
    "        Helper Function:\n",
    "        Extracts phrases from the alignment output for a single sentence.\n",
    "        For unaligned phrases, creates a phrase pair where null side is None.\n",
    "        \"\"\"\n",
    "        srclen = len(srctext.split())\n",
    "        trglen = len(trgtext.split())\n",
    "        srctext_lst = srctext.split()\n",
    "        trgtext_lst = trgtext.split()\n",
    "        src_dict = {}\n",
    "        trg_dict = {}\n",
    "        for i, word in enumerate(srctext.split()):\n",
    "            src_dict[i] = word\n",
    "        for i, word in enumerate(trgtext.split()):\n",
    "            trg_dict[i] = word\n",
    "        f_aligned = set()\n",
    "        e_aligned = set()\n",
    "        line_lst = alignment_output_line.split()\n",
    "        alignment = list()\n",
    "        for pair in line_lst:\n",
    "            [f, e] = pair.split(\"-\")\n",
    "            f_aligned.add(int(f))\n",
    "            e_aligned.add(int(e))\n",
    "            alignment.append((int(f),int(e)))\n",
    "        alignment = sorted(alignment)\n",
    "\n",
    "        phrase_set = set()\n",
    "        phrase = set()\n",
    "        prev_e = 0\n",
    "        prev_f = 0\n",
    "        for f,e in alignment:\n",
    "            if abs(e - prev_e) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            elif abs(f - prev_f) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            else:\n",
    "                phrase.add((e,f))\n",
    "            prev_e = e\n",
    "            prev_f = f\n",
    "        phrase_set.add(frozenset(phrase))\n",
    "\n",
    "        full_phrases = []\n",
    "        for i in sorted(phrase_set):\n",
    "            if len(i) > 0:\n",
    "                phrase_e = set()\n",
    "                phrase_f = set()\n",
    "                for pair in sorted(i):\n",
    "                    phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                    phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "                phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "                full_phrases.append(phrase_pair)\n",
    "        unaligned_phrases = []\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[0][0])):\n",
    "            #print(i,phrase_pair[0],phrase_pair[0][-1][0],phrase_pair[1])\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[0][-1][0] != phrase_pair[0][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[0][-1][0]+1,phrase_pair[0][0][0]):\n",
    "                        unaligned_sequence.append((k,trgtext_lst[k]))\n",
    "                    unaligned_phrases.append([unaligned_sequence,None])\n",
    "                prev_pair = phrase_pair\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[1][0])):\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[1][-1][0] != phrase_pair[1][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[1][-1][0]+1,phrase_pair[1][0][0]):\n",
    "                        unaligned_sequence.append((k,srctext_lst[k]))\n",
    "                    unaligned_phrases.append([None,unaligned_sequence])\n",
    "                prev_pair = phrase_pair\n",
    "        full_phrases.extend(unaligned_phrases)\n",
    "        \n",
    "        full_phrase_set = set()\n",
    "        for i in full_phrases:\n",
    "            new_e = tuple(i[0]) if i[0] else None\n",
    "            new_f = tuple(i[1]) if i[1] else None\n",
    "            new_phrase = (new_e, new_f)\n",
    "            full_phrase_set.add(new_phrase)\n",
    "            \n",
    "        return full_phrase_set\n",
    "        # end function extract_phrases\n",
    "    \n",
    "    # Main code within function initialize_tlmo()\n",
    "    with open(srctext_file,\"r\",encoding=\"utf-8\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\",encoding=\"utf-8\") as trgtext_f, \\\n",
    "    open(alignments_file,\"r\") as alignments_f:\n",
    "        for i, (sline,tline,aline) in enumerate(zip(srctext_f,trgtext_f,alignments_f)):\n",
    "            src = sline.strip(\"\\n\\r\")\n",
    "            trg = tline.strip(\"\\n\\r\")\n",
    "            A = aline.strip(\"\\n\\r\")\n",
    "            full_phrase_set = extract_phrases(src,trg,A)\n",
    "            sent_pair = SentencePair(i,trg,src,full_phrase_set)\n",
    "            TLMO.init_vocab_from_text(\"e\",trg)\n",
    "            TLMO.init_vocab_from_text(\"f\",src)\n",
    "            TLMO.init_pp_counts_from_pair_set(sent_pair.get_alignment())\n",
    "            TLMO.set_sent_pair(sent_pair)\n",
    "    \n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapsed_gibbs_aligner(srctext_file,trgtext_file,TLMO,hp):\n",
    "    \"\"\"\n",
    "    Will want to repeatedly call this for N iterations.\n",
    "    \"\"\"\n",
    "    # Supporting functions:\n",
    "    def p_f(src_phrase,n_f):\n",
    "        return (p_s*(1-p_s)**(len(src_phrase)))*((1/n_f)**(len(src_phrase)))\n",
    "    \n",
    "    def p_e(trg_phrase,n_e):\n",
    "        return (p_s*(1-p_s)**(len(trg_phrase)))*((1/n_e)**(len(trg_phrase)))\n",
    "    \n",
    "    def calc_P_WA_trg_src(sent_ind, trg_phrase, sline):\n",
    "        \"\"\"\n",
    "        Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "        if looking for P_WA(e|f), as in this function, trg_phrase is [(0,\"Michael\"),(1,\"assumes\")]\n",
    "        and sline is the source sentence in the form of a list: [\"Michael\", \"geht\", \"davon\", \"aus\", ...]\n",
    "        Should use dynamic programming to store calculations of phrase probabilities\n",
    "        based on IBM Model 1 word alignment probabilities.\n",
    "        \"\"\"\n",
    "        for i, pair in enumerate(trg_phrase):\n",
    "            # trg_phrase = ((0, 'This'), (1, 'is'))\n",
    "            # i = 0, pair = (0, 'This')\n",
    "            # i = 1, pair = (1, 'is')\n",
    "            if i in trg_align_dict[sent_ind][pair[0]]:\n",
    "                for word_ind in trg_align_dict[sent_ind][pair[0]]:\n",
    "                    if i == 0:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if sline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value = P_WA[pair[1]][sline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value = lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value = lowest_prob\n",
    "                    else:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if sline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value *= P_WA[pair[1]][sline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value *= lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value *= lowest_prob\n",
    "            else:\n",
    "                P_WA_value = lowest_prob\n",
    "        return P_WA_value\n",
    "    \n",
    "    def calc_P_WA_src_trg(sent_ind, src_phrase, tline):\n",
    "        \"\"\"\n",
    "        Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "        if looking for P_WA(f|e), as in this function, src_phrase is [(0,\"Michael\"),(1,\"geht\"),(2,\"davon\"),(3,\"aus\")]\n",
    "        and tline is the target sentence in the form of a list: [\"Michael\", \"assumes\", ...]\n",
    "        Should use dynamic programming to store calculations of phrase probabilities\n",
    "        based on IBM Model 1 word alignment probabilities. \n",
    "        \"\"\"\n",
    "        for i, pair in enumerate(src_phrase):\n",
    "            if i in src_align_dict[sent_ind][pair[0]]:\n",
    "                for word_ind in src_align_dict[sent_ind][pair[0]]:\n",
    "                    if i == 0:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if tline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value = P_WA[pair[1]][tline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value = lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value = lowest_prob\n",
    "                    else:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if tline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value *= P_WA[pair[1]][tline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value *= lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value *= lowest_prob\n",
    "            else:\n",
    "                P_WA_value = lowest_prob\n",
    "        return P_WA_value\n",
    "    \n",
    "    def delta(trg_phrase,src_phrase,tline,sline):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        s = float(len(tline))/float(len(sline))\n",
    "        delta = b**abs(trg_phrase[0][0]-(src_phrase[0][0]*s))\n",
    "        return delta\n",
    "    \n",
    "    def tau(trg_phrase,src_phrase,phrase_counts):\n",
    "        tau_value = (phrase_counts[(trg_phrase,src_phrase)] + \\\n",
    "                     (alpha * ((p_f(src_phrase,n_f) * \\\n",
    "                              calc_P_WA_trg_src(sent_ind, trg_phrase, sline)) * \\\n",
    "                             (p_e(trg_phrase,n_e) * \\\n",
    "                             calc_P_WA_src_trg(sent_ind, src_phrase, tline))))) / \\\n",
    "        (len(phrase_counts) + alpha)\n",
    "        return tau_value\n",
    "    \n",
    "    def theta_N(trg_phrase,src_phrase):\n",
    "        if trg_phrase and not src_phrase:\n",
    "            theta_N_value = 0.5*p_e(trg_phrase,n_e)\n",
    "        elif src_phrase and not trg_phrase:\n",
    "            theta_N_value = 0.5*p_f(src_phrase,n_f)\n",
    "        return theta_N_value\n",
    "    \n",
    "    def objective(trg_phrase,src_phrase,phrase_counts,tline,sline,sent_ind):\n",
    "        \"\"\"\n",
    "        This calculates the potential function's value.\n",
    "        Watch out! Need to be able to modify to accommodate Markov blanket...\n",
    "        \"\"\"\n",
    "        if src_phrase and trg_phrase:\n",
    "            obj_value = (1-p_S)*(1-p_phi)*tau(trg_phrase,src_phrase,phrase_counts)*delta(trg_phrase,src_phrase,tline,sline)\n",
    "        else:\n",
    "            obj_value = (1-p_S)*p_phi*theta_N(trg_phrase,src_phrase)\n",
    "\n",
    "        return obj_value\n",
    "    \n",
    "    def create_phrase_map(sline,tline,full_phrases):\n",
    "        \"\"\"\n",
    "        full_phrases is the value of a key in the dictionary phrase_table, e.g. phrase_tables[0]\n",
    "        to get the full_phrases of the first sentence.\n",
    "        \"\"\"\n",
    "        src_phrase_map = {}\n",
    "        trg_phrase_map = {}\n",
    "        for phrase_ind, pair in enumerate(full_phrases):\n",
    "            trg = pair[0]\n",
    "            src = pair[1]\n",
    "            if trg:\n",
    "                for entry in trg:\n",
    "                    trg_phrase_map[entry[0]] = trg\n",
    "            if src:\n",
    "                for entry in src:\n",
    "                    src_phrase_map[entry[0]] = src\n",
    "        for src_ind, word in enumerate(sline):\n",
    "            if src_ind not in src_phrase_map:\n",
    "                src_phrase_map[src_ind] = None\n",
    "        for trg_ind, word in enumerate(tline):\n",
    "            if trg_ind not in trg_phrase_map:\n",
    "                trg_phrase_map[trg_ind] = None\n",
    "        return (src_phrase_map, trg_phrase_map)\n",
    "    \n",
    "    def SWAP(e_phrase,f_phrase,e_phrase_2,f_phrase_2,sent_pair,TLMO):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize scorekeeping dict\n",
    "        pair_score = dict()\n",
    "        # Set up configurations as tuples. Names of variables will be keys in dict.\n",
    "        ident = (e_phrase,f_phrase,e_phrase_2,f_phrase_2)\n",
    "        swapped = (e_phrase,f_phrase_2,e_phrase_2,f_phrase)\n",
    "        out_conf_dict = {\"ident\":ident,\"swapped\":swapped}\n",
    "        # First, modifying counts and alignments so that TLMO and sent_pair are the Markov blanket.\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase))\n",
    "        sent_pair.remove_alignment((e_phrase,f_phrase))\n",
    "        sent_pair.remove_alignment((e_phrase_2,f_phrase_2))\n",
    "        # Do I need to decrement the counts for the swapped configuration?\n",
    "        # Second, calculate objective scores.\n",
    "        #     First, swapped configuration: increment phrase counts for it, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        TLMO.increment_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.increment_pp_counts((e_phrase_2,f_phrase))\n",
    "        pair_score[\"swapped\"] = (objective(e_phrase,f_phrase_2,TLMO,sent_pair),\n",
    "                                 objective(e_phrase_2,f_phrase,TLMO,sent_pair))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase_2))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase))\n",
    "        #     Second, identify configuration: increment phrase_counts for identity, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        TLMO.increment_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.increment_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        pair_score[\"ident\"] = (objective(e_phrase,f_phrase,TLMO,sent_pair),\n",
    "                               objective(e_phrase_2,f_phrase_2,TLMO,sent_pair))\n",
    "        TLMO.decrement_pp_counts((e_phrase,f_phrase))\n",
    "        TLMO.decrement_pp_counts((e_phrase_2,f_phrase_2))\n",
    "        # calculate output configuration probabilities\n",
    "        ident_prob = (pair_score[\"ident\"][0]*pair_score[\"ident\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        swapped_prob = (pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        # choose one configuration probabilistically\n",
    "        out_conf_decision = np.random.choice([\"ident\",\"swapped\"],1,p=[ident_prob,swapped_prob])[0]\n",
    "        out_conf = out_conf_dict[out_conf_decision]\n",
    "        # update phrase_counts, phrase_table depending on out_conf, which is a tuple of (f,e,f,e) phrases\n",
    "        TLMO.increment_pp_counts((out_conf[0],out_conf[1]))\n",
    "        TLMO.increment_pp_counts((out_conf[2],out_conf[3]))\n",
    "        # update alignments\n",
    "        sent_pair.add_alignment((out_conf[0],out_conf[1]))\n",
    "        sent_pair.add_alignment((out_conf[2],out_conf[3]))\n",
    "        TLMO.set_sent_pair(sent_ind,sent_pair)\n",
    "        \n",
    "    def FLIP():\n",
    "        pass\n",
    "    \n",
    "    def TOGGLE(src_ph,trg_ph,phrase_counts,phrase_table):\n",
    "        # WORK HERE NEXT - convert to make reference to fancy new objects!\n",
    "        pair_score = dict()\n",
    "        prob_score[] = dict()\n",
    "        if src_ph_1 != None and trg_ph_1 != None:\n",
    "            delinked = ((trg_ph_1,None),(None,src_ph_1))\n",
    "            ident = (trg_ph_1,src_ph_1)\n",
    "            # Take out phrase_count for this pair to accommodate markov blanket\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            # see what effect delinking (i.e. \"unaligning\") them has\n",
    "            pair_score[\"delinked\"] = objective(trg_ph_1,None,phrase_counts,tline,sline,sent_ind)*objective(None,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # Augment phrase_counts to capture \"new\" pairing (which was originally there, in this case)\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) += 1\n",
    "            pair_score[\"ident\"] = objective(trg_ph_1,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # Restore Markov blanket, and await decision as to shape this should take below.\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            prob_score[\"ident\"] = pair_score[\"ident\"]/(pair_score[\"ident\"]+pair_score[\"delinked\"])\n",
    "            prob_score[\"delinked\"] = pair_score[\"delinked\"]/(pair_score[\"ident\"]+pair_score[\"delinked\"])\n",
    "            out_conf_decision = np.random.choice([\"ident\",\"delinked\"],1,p=[prob_score[\"ident\"],prob_score[\"delinked\"]])\n",
    "            if out_conf_decision == \"ident\":\n",
    "                phrase_counts((trg_ph_1,src_ph_1)) += 1\n",
    "            elif out_conf_decision == \"delinked\":\n",
    "                phrase_table[sent_ind].remove((trg_ph_1,src_ph_1))\n",
    "        elif src_ph_1 == None and trg_ph_1 == None:\n",
    "            # create Markov blanket\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            # see what effect aligning them has\n",
    "            aligned = (trg_ph_1,src_ph_1)\n",
    "            pair_score[\"delinked\"] = objective(trg_ph_1,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # START WORKING HERE ON 1/13\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return (phrase_counts, phrase_table)\n",
    "    \n",
    "    \n",
    "    # Main code for collapsed Gibbs sampler\n",
    "    with open(srctext_file,\"r\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\") as trgtext_f:\n",
    "        for sent_ind, (f_line, e_line) in enumerate(zip(srctext_f,trgtext_f)):\n",
    "            eline = e_line.strip(\"\\n\\r\").split()\n",
    "            fline = f_line.strip(\"\\n\\r\").split()\n",
    "            for f_ind, f_word in enumerate(fline):\n",
    "                for e_ind, e_word in enumerate(eline):\n",
    "                    # Apply SWAP, if applicable - could remove some of these for-loops with a dictionary \n",
    "                    # of phrase alignments.\n",
    "                    # CHECK TO ENSURE THAT WE'RE NOT MISSING ANY PHRASES IN EITHER LANGUAGE!\n",
    "                    sent_pair = TLMO.get_sent_pair(sent_ind)\n",
    "                    e_phrase = sent_pair.query_phrase_map(\"e\",e_ind)\n",
    "                    f_phrase = sent_pair.query_alignment_dict(\"e-f\",e_ph)\n",
    "                    if e_prev and e_phrase != e_prev:\n",
    "                        sorted_e_ph = sorted(sent_pair.get_e_phrases())\n",
    "                        next_e_ind = (sorted_e_ph.index(e_phrase))+1\n",
    "                        if next_e_ind <= len(sorted_e_ph):\n",
    "                            e_phrase_2 = sorted_e_ph[next_e_ind]\n",
    "                            f_phrase_2 = sent_pair.query_alignment_dict(\"e-f\",e_phrase_2)\n",
    "                            # actual call to SWAP.\n",
    "                            SWAP(sent_ind,e_phrase,f_phrase,e_phrase_2,f_phrase_2,sent_pair,TLMO)\n",
    "                            e_prev = e_phrase\n",
    "                            f_prev = f_phrase\n",
    "                    # else, don't do SWAP!\n",
    "\n",
    "                    # Apply TOGGLE, if applicable\n",
    "                    #TOGGLE()\n",
    "                    # Apply FLIP, if applicable\n",
    "                    \n",
    "                    \n",
    "                    # Apply FLIP TWO, if applicable\n",
    "                    \n",
    "                    # Apply MOVE, if applicable\n",
    "                    \n",
    "#            final_probability = 0   # PLACEHOLDER - fill with model calculation.\n",
    "    return TLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 'michael'), (0, 'something else')), ((3, 'lol'), (2, 'god...'))]\n"
     ]
    }
   ],
   "source": [
    "print(sorted((((3,\"lol\"),(2,\"god...\")),((1,\"michael\"),(0,\"something else\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"main\":\n",
    "    # Use argparse to allow user to enter hyperparameters and input files\n",
    "    hp = Hyperparameters(p_S,b,p_phi,alpha,p_s)\n",
    "    TLMO = TextLevelMutableObjects    # Initialize TLMO to contain what would have been global variables (AHHH!)\n",
    "    TLMO = process_IBM_Model_1_data(src_vocab_file,trg_vocab_file,src_trg_alignment_file,trg_src_alignment_file,TLMO)\n",
    "    TLMO = initialize_tlmo(srctext_file,trgtext_file,alignments_file,TLMO)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
