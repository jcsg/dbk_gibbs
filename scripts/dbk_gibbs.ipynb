{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Make sure that you include unaligned phrases as phrases, not just gaps in coverage.\n",
    "# It's very important that *everything be in a phrase*, even if it's unaligned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict as dd\n",
    "import copy\n",
    "import unicodecsv as csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_one_processing(src_vocab_file,trg_vocab_file,src_trg_alignment,trg_src_alignment):\n",
    "    \"\"\"\n",
    "    Takes the output of IBM Model 1 alignments from GIZA++ and creates a lookup\n",
    "    table for the translation probabilities of f|e and e|f.\n",
    "    Here, src = foreign (f), trg = english (e)\n",
    "    Returns a lookup dictionary of the form P_WA[word1][word2] = alignment probability of word1|word2\n",
    "    # Is this correct given how GIZA++ does things?\n",
    "    \"\"\"\n",
    "    src_vocab_lut = {}\n",
    "    trg_vocab_lut = {}\n",
    "    P_WA = dd(dict)\n",
    "    lowest_prob = 1.0\n",
    "    with open(src_vocab_file,\"rb\") as src_vocab_f:\n",
    "        src_vocab = csv.reader(src_vocab_f,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in src_vocab:\n",
    "            if len(line) == 3:\n",
    "                src_vocab_lut[int(line[0])] = line[1]\n",
    "    with open(trg_vocab_file,\"rb\") as trg_vocab_f:\n",
    "        trg_vocab = csv.reader(trg_vocab_f,delimiter=\" \",encoding=\"utf-8\")\n",
    "        for line in trg_vocab:\n",
    "            if len(line) == 3:\n",
    "                trg_vocab_lut[int(line[0])] = line[1]\n",
    "    with open(src_trg_alignment,\"rb\") as src_trg_alignment_f:\n",
    "        src_trg_alignment_loop = csv.reader(src_trg_alignment_f,delimiter=\" \")\n",
    "        for line in src_trg_alignment_loop:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in src_vocab_lut and int(line[1]) in trg_vocab_lut:\n",
    "                    P_WA[src_vocab_lut[int(line[0])]][trg_vocab_lut[int(line[1])]] = float(line[2])\n",
    "                    if float(line[2]) < lowest_prob:\n",
    "                        lowest_prob = float(line[2])\n",
    "    with open(trg_src_alignment,\"rb\") as trg_src_alignment_f:\n",
    "        trg_src_alignment_loop = csv.reader(trg_src_alignment_f,delimiter=\" \")\n",
    "        for line in trg_src_alignment_loop:\n",
    "            if len(line) == 3:\n",
    "                if int(line[0]) in trg_vocab_lut and int(line[1]) in src_vocab_lut:\n",
    "                    P_WA[trg_vocab_lut[int(line[0])]][src_vocab_lut[int(line[1])]] = float(line[2])\n",
    "                    if float(line[2]) < lowest_prob:\n",
    "                        lowest_prob = float(line[2])\n",
    "    return (P_WA, lowest_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_phrases_from_text(srctext_file,trgtext_file,alignments_file):\n",
    "    \"\"\"\n",
    "    For now, these are the input files to the Berkeley aligner, as well as \n",
    "    the output training.align file for alignments_file.\n",
    "    Assumes that the two text input files have the same number of lines, and that these all have alignment output.\n",
    "    Assume src = Foreign (f) and trg = English (e)\n",
    "    \"\"\"\n",
    "    def extract_phrases(srctext,trgtext,alignment_output_line):\n",
    "        \"\"\"\n",
    "        Extracts phrases from the alignment output for a single sentence.\n",
    "        For unaligned phrases, creates a phrase pair where null side is None.\n",
    "        \"\"\"\n",
    "        srclen = len(srctext.split())\n",
    "        trglen = len(trgtext.split())\n",
    "        srctext_lst = srctext.split()\n",
    "        trgtext_lst = trgtext.split()\n",
    "        src_dict = {}\n",
    "        trg_dict = {}\n",
    "        for i, word in enumerate(srctext.split()):\n",
    "            src_dict[i] = word\n",
    "        for i, word in enumerate(trgtext.split()):\n",
    "            trg_dict[i] = word\n",
    "        f_aligned = set()\n",
    "        e_aligned = set()\n",
    "        line_lst = alignment_output_line.split()\n",
    "        alignment = list()\n",
    "        for pair in line_lst:\n",
    "            [f, e] = pair.split(\"-\")\n",
    "            f_aligned.add(int(f))\n",
    "            e_aligned.add(int(e))\n",
    "            alignment.append((int(f),int(e)))\n",
    "        alignment = sorted(alignment)\n",
    "\n",
    "        phrase_set = set()\n",
    "        phrase = set()\n",
    "        prev_e = 0\n",
    "        prev_f = 0\n",
    "        for f,e in alignment:\n",
    "            if abs(e - prev_e) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            elif abs(f - prev_f) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            else:\n",
    "                phrase.add((e,f))\n",
    "            prev_e = e\n",
    "            prev_f = f\n",
    "        phrase_set.add(frozenset(phrase))\n",
    "\n",
    "        full_phrases = []\n",
    "        for i in sorted(phrase_set):\n",
    "            if len(i) > 0:\n",
    "                phrase_e = set()\n",
    "                phrase_f = set()\n",
    "                for pair in sorted(i):\n",
    "                    phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                    phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "                phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "                full_phrases.append(phrase_pair)\n",
    "        unaligned_phrases = []\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[0][0])):\n",
    "            #print(i,phrase_pair[0],phrase_pair[0][-1][0],phrase_pair[1])\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[0][-1][0] != phrase_pair[0][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[0][-1][0]+1,phrase_pair[0][0][0]):\n",
    "                        unaligned_sequence.append((k,trgtext_lst[k]))\n",
    "                    unaligned_phrases.append([unaligned_sequence,None])\n",
    "                prev_pair = phrase_pair\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[1][0])):\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[1][-1][0] != phrase_pair[1][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[1][-1][0]+1,phrase_pair[1][0][0]):\n",
    "                        unaligned_sequence.append((k,srctext_lst[k]))\n",
    "                    unaligned_phrases.append([None,unaligned_sequence])\n",
    "                prev_pair = phrase_pair\n",
    "        full_phrases.extend(unaligned_phrases)\n",
    "        \n",
    "        full_phrase_set = set()\n",
    "        for i in full_phrases:\n",
    "            new_e = tuple(i[0]) if i[0] else None\n",
    "            new_f = tuple(i[1]) if i[1] else None\n",
    "            new_phrase = (new_e, new_f)\n",
    "            full_phrase_set.add(new_phrase)\n",
    "            \n",
    "        return full_phrase_set\n",
    "    \n",
    "    phrase_counts = Counter()\n",
    "    phrase_table = {}\n",
    "    src_align_dict = {}\n",
    "    trg_align_dict = {}\n",
    "    unigram_e_len = Counter()\n",
    "    unigram_f_len = Counter()\n",
    "    n_e_set = set()\n",
    "    n_f_set = set()\n",
    "    with open(srctext_file,\"r\",encoding=\"utf-8\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\",encoding=\"utf-8\") as trgtext_f, \\\n",
    "    open(alignments_file,\"r\") as alignments_f:\n",
    "        for i, (sline,tline,aline) in enumerate(zip(srctext_f,trgtext_f,alignments_f)):\n",
    "            src = sline.strip(\"\\n\\r\")\n",
    "            for word in src.split():\n",
    "                n_f_set.add(word)\n",
    "            trg = tline.strip(\"\\n\\r\")\n",
    "            for word in trg.split():\n",
    "                n_e_set.add(word)\n",
    "            A = aline.strip(\"\\n\\r\")\n",
    "            full_phrases = extract_phrases(src,trg,A)\n",
    "            phrase_table[i] = full_phrases\n",
    "            for phrase_pair in full_phrases:\n",
    "                if len(phrase_pair) == 2:\n",
    "                    trg_phrase_lst = []\n",
    "                    src_phrase_lst = []\n",
    "                    if phrase_pair[0]:\n",
    "                        target = phrase_pair[0]\n",
    "                        #print(\"target =\", target, type(target))\n",
    "                        unigram_e_len[target] += len(target)\n",
    "                        for ind,word in target:\n",
    "                            trg_phrase_lst.append(word)\n",
    "                        trg_phrase = \" \".join(trg_phrase_lst)\n",
    "                    else:\n",
    "                        trg_phrase = None\n",
    "                    if phrase_pair[1]:\n",
    "                        source = phrase_pair[1]\n",
    "                        #print(\"source =\", source, type(source), len(source))\n",
    "                        unigram_f_len[source] += len(source)\n",
    "                        for ind,word in source:\n",
    "                            src_phrase_lst.append(word)\n",
    "                        src_phrase = \" \".join(src_phrase_lst)\n",
    "                    else:\n",
    "                        src_phrase = None\n",
    "                    phrase_counts[(trg_phrase,src_phrase)] += 1\n",
    "            src_align_dict[i] = dd(set)\n",
    "            trg_align_dict[i] = dd(set)\n",
    "            for align_pair in A.split():\n",
    "                pair = align_pair.split(\"-\")\n",
    "                src_ind = int(pair[0])\n",
    "                trg_ind = int(pair[1])\n",
    "                src_align_dict[i][src_ind].add(trg_ind)\n",
    "                trg_align_dict[i][trg_ind].add(src_ind)\n",
    "        n_e = len(n_e_set)\n",
    "        n_f = len(n_f_set)\n",
    "        \n",
    "    return (phrase_counts, phrase_table, src_align_dict, trg_align_dict, unigram_e_len, unigram_f_len, n_e, n_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-598f9ccf5435>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-598f9ccf5435>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def extract_phrases(srctext,trgtext,alignment_output_line):\n",
    "        \"\"\"\n",
    "        #WORKING: List Version!\n",
    "        #Extracts phrases from the alignment output for a single sentence.\n",
    "        #For unaligned phrases, creates a phrase pair where null side is None.\n",
    "        \"\"\"\n",
    "        srclen = len(srctext.split())\n",
    "        trglen = len(trgtext.split())\n",
    "        srctext_lst = srctext.split()\n",
    "        trgtext_lst = trgtext.split()\n",
    "        src_dict = {}\n",
    "        trg_dict = {}\n",
    "        for i, word in enumerate(srctext.split()):\n",
    "            src_dict[i] = word\n",
    "        for i, word in enumerate(trgtext.split()):\n",
    "            trg_dict[i] = word\n",
    "        f_aligned = set()\n",
    "        e_aligned = set()\n",
    "        line_lst = alignment_output_line.split()\n",
    "        alignment = list()\n",
    "        for pair in line_lst:\n",
    "            [f, e] = pair.split(\"-\")\n",
    "            f_aligned.add(int(f))\n",
    "            e_aligned.add(int(e))\n",
    "            alignment.append((int(f),int(e)))\n",
    "        alignment = sorted(alignment)\n",
    "\n",
    "        phrase_set = set()\n",
    "        phrase = set()\n",
    "        prev_e = 0\n",
    "        prev_f = 0\n",
    "        for f,e in alignment:\n",
    "            if abs(e - prev_e) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            elif abs(f - prev_f) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            else:\n",
    "                phrase.add((e,f))\n",
    "            prev_e = e\n",
    "            prev_f = f\n",
    "        phrase_set.add(frozenset(phrase))\n",
    "\n",
    "        full_phrases = []\n",
    "        for i in sorted(phrase_set):\n",
    "            if len(i) > 0:\n",
    "                phrase_e = set()\n",
    "                phrase_f = set()\n",
    "                for pair in sorted(i):\n",
    "                    phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                    phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "                phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "                full_phrases.append(phrase_pair)\n",
    "        unaligned_phrases = []\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[0][0])):\n",
    "            #print(i,phrase_pair[0],phrase_pair[0][-1][0],phrase_pair[1])\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[0][-1][0] != phrase_pair[0][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[0][-1][0]+1,phrase_pair[0][0][0]):\n",
    "                        unaligned_sequence.append((k,trgtext_lst[k]))\n",
    "                    unaligned_phrases.append([unaligned_sequence,None])\n",
    "                prev_pair = phrase_pair\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[1][0])):\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[1][-1][0] != phrase_pair[1][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[1][-1][0]+1,phrase_pair[1][0][0]):\n",
    "                        unaligned_sequence.append((k,srctext_lst[k]))\n",
    "                    unaligned_phrases.append([None,unaligned_sequence])\n",
    "                prev_pair = phrase_pair\n",
    "        full_phrases.extend(unaligned_phrases)\n",
    "\n",
    "        return full_phrases\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((0, 'Michael'), (1, 'assumes')), ((0, 'Michael'), (1, 'geht'), (2, 'davon'), (3, 'aus')))\n",
      "(((4, 'will'), (5, 'stay')), ((9, 'bleibt'),))\n",
      "(((2, 'that'), (3, 'he')), ((5, 'dass'), (6, 'er')))\n",
      "(None, ((4, ','),))\n",
      "(((6, 'in'), (7, 'the'), (8, 'house')), ((7, 'im'), (8, 'haus')))\n"
     ]
    }
   ],
   "source": [
    "# QUICK TESTER CODE just for Extract function, now private within \n",
    "# initialize_phrases_from_text function\n",
    "\n",
    "def extract_phrases_test(srctext,trgtext,alignment_output_line):\n",
    "        \"\"\"\n",
    "        Extracts phrases from the alignment output for a single sentence.\n",
    "        For unaligned phrases, creates a phrase pair where null side is None.\n",
    "        \"\"\"\n",
    "        srclen = len(srctext.split())\n",
    "        trglen = len(trgtext.split())\n",
    "        srctext_lst = srctext.split()\n",
    "        trgtext_lst = trgtext.split()\n",
    "        src_dict = {}\n",
    "        trg_dict = {}\n",
    "        for i, word in enumerate(srctext.split()):\n",
    "            src_dict[i] = word\n",
    "        for i, word in enumerate(trgtext.split()):\n",
    "            trg_dict[i] = word\n",
    "        f_aligned = set()\n",
    "        e_aligned = set()\n",
    "        line_lst = alignment_output_line.split()\n",
    "        alignment = list()\n",
    "        for pair in line_lst:\n",
    "            [f, e] = pair.split(\"-\")\n",
    "            f_aligned.add(int(f))\n",
    "            e_aligned.add(int(e))\n",
    "            alignment.append((int(f),int(e)))\n",
    "        alignment = sorted(alignment)\n",
    "\n",
    "        phrase_set = set()\n",
    "        phrase = set()\n",
    "        prev_e = 0\n",
    "        prev_f = 0\n",
    "        for f,e in alignment:\n",
    "            if abs(e - prev_e) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            elif abs(f - prev_f) > 1:\n",
    "                phrase_set.add(frozenset(phrase))\n",
    "                phrase = set([(e,f)])\n",
    "            else:\n",
    "                phrase.add((e,f))\n",
    "            prev_e = e\n",
    "            prev_f = f\n",
    "        phrase_set.add(frozenset(phrase))\n",
    "\n",
    "        full_phrases = []\n",
    "        for i in sorted(phrase_set):\n",
    "            if len(i) > 0:\n",
    "                phrase_e = set()\n",
    "                phrase_f = set()\n",
    "                for pair in sorted(i):\n",
    "                    phrase_e.add((pair[0],trg_dict[pair[0]]))\n",
    "                    phrase_f.add((pair[1],src_dict[pair[1]]))\n",
    "                phrase_pair = [sorted(phrase_e),sorted(phrase_f)]\n",
    "                full_phrases.append(phrase_pair)\n",
    "        unaligned_phrases = []\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[0][0])):\n",
    "            #print(i,phrase_pair[0],phrase_pair[0][-1][0],phrase_pair[1])\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[0][-1][0] != phrase_pair[0][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[0][-1][0]+1,phrase_pair[0][0][0]):\n",
    "                        unaligned_sequence.append((k,trgtext_lst[k]))\n",
    "                    unaligned_phrases.append([unaligned_sequence,None])\n",
    "                prev_pair = phrase_pair\n",
    "        for i, phrase_pair in enumerate(sorted(full_phrases,key=lambda x:x[1][0])):\n",
    "            if i == 0:\n",
    "                prev_pair = phrase_pair\n",
    "                continue\n",
    "            else:\n",
    "                if prev_pair[1][-1][0] != phrase_pair[1][0][0]-1:\n",
    "                    unaligned_sequence = []\n",
    "                    for k in range(prev_pair[1][-1][0]+1,phrase_pair[1][0][0]):\n",
    "                        unaligned_sequence.append((k,srctext_lst[k]))\n",
    "                    unaligned_phrases.append([None,unaligned_sequence])\n",
    "                prev_pair = phrase_pair\n",
    "        full_phrases.extend(unaligned_phrases)\n",
    "        \n",
    "        full_phrase_set = set()\n",
    "        for i in full_phrases:\n",
    "            new_e = tuple(i[0]) if i[0] else None\n",
    "            new_f = tuple(i[1]) if i[1] else None\n",
    "            new_phrase = (new_e, new_f)\n",
    "            full_phrase_set.add(new_phrase)\n",
    "            \n",
    "        return full_phrase_set\n",
    "\n",
    "srctext = \"Michael geht davon aus , dass er im haus bleibt\"\n",
    "trgtext = \"Michael assumes that he will stay in the house\"\n",
    "alignment_output_line = \"0-0 1-1 2-1 3-1 5-2 6-3 7-6 7-7 8-8 9-4 9-5\"\n",
    "\n",
    "full_phrase_set = extract_phrases_test(srctext,trgtext,alignment_output_line)\n",
    "for i in full_phrase_set:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-06e121e7207f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "print(len((None, ((4, ','),))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Supporting functions:\n",
    "def p_f(src_phrase,n_f):\n",
    "    return (p_s*(1-p_s)**(len(src_phrase)))*((1/n_f)**(len(src_phrase)))\n",
    "\n",
    "def p_e(trg_phrase,n_e):\n",
    "    return (p_s*(1-p_s)**(len(trg_phrase)))*((1/n_e)**(len(trg_phrase)))\n",
    "\n",
    "def calc_P_WA_trg_src(sent_ind, trg_phrase, sline):\n",
    "    \"\"\"\n",
    "    Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "    if looking for P_WA(e|f), as in this function, trg_phrase is [(0,\"Michael\"),(1,\"assumes\")]\n",
    "    and sline is the source sentence in the form of a list: [\"Michael\", \"geht\", \"davon\", \"aus\", ...]\n",
    "    Should use dynamic programming to store calculations of phrase probabilities\n",
    "    based on IBM Model 1 word alignment probabilities.\n",
    "    \"\"\"\n",
    "    for i, pair in enumerate(trg_phrase):\n",
    "        if word_ind in trg_align_dict[sent_ind][pair[0]]:\n",
    "            for word_ind in trg_align_dict[sent_ind][pair[0]]:\n",
    "                if i == 0:\n",
    "                    if pair[1] in P_WA:\n",
    "                        if sline[word_ind] in P_WA[pair[1]]:\n",
    "                            P_WA_value = P_WA[pair[1]][sline[word_ind]]\n",
    "                        else:\n",
    "                            P_WA_value = 0\n",
    "                    else:\n",
    "                        P_WA_value = 0\n",
    "                else:\n",
    "                    if pair[1] in P_WA:\n",
    "                        if sline[word_ind] in P_WA[pair[1]]:\n",
    "                            P_WA_value *= P_WA[pair[1]][sline[word_ind]]\n",
    "                        else:\n",
    "                            P_WA_value *= 0\n",
    "                    else:\n",
    "                        P_WA_value *= 0\n",
    "        else:\n",
    "            P_WA_value = 0\n",
    "    return P_WA_value\n",
    "\n",
    "def calc_P_WA_src_trg(sent_ind, src_phrase, tline):\n",
    "    \"\"\"\n",
    "    Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "    if looking for P_WA(f|e), as in this function, src_phrase is [(0,\"Michael\"),(1,\"geht\"),(2,\"davon\"),(3,\"aus\")]\n",
    "    and tline is the target sentence in the form of a list: [\"Michael\", \"assumes\", ...]\n",
    "    Should use dynamic programming to store calculations of phrase probabilities\n",
    "    based on IBM Model 1 word alignment probabilities. \n",
    "    \"\"\"\n",
    "    for i, pair in enumerate(src_phrase):\n",
    "        if word_ind in src_align_dict[sent_ind][pair[0]]:\n",
    "            for word_ind in src_align_dict[sent_ind][pair[0]]:\n",
    "                if i == 0:\n",
    "                    if pair[1] in P_WA:\n",
    "                        if tline[word_ind] in P_WA[pair[1]]:\n",
    "                            P_WA_value = P_WA[pair[1]][tline[word_ind]]\n",
    "                        else:\n",
    "                            P_WA_value = 0\n",
    "                    else:\n",
    "                        P_WA_value = 0\n",
    "                else:\n",
    "                    if pair[1] in P_WA:\n",
    "                        if tline[word_ind] in P_WA[pair[1]]:\n",
    "                            P_WA_value *= P_WA[pair[1]][tline[word_ind]]\n",
    "                        else:\n",
    "                            P_WA_value *= 0\n",
    "                    else:\n",
    "                        P_WA_value *= 0\n",
    "        else:\n",
    "            P_WA_value = 0\n",
    "    return P_WA_value\n",
    "\n",
    "def delta(trg_phrase,src_phrase,tline,sline):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    s = float(len(tline))/float(len(sline))\n",
    "    delta = b**abs(trg_phrase[0][0]-(src_phrase[0][0]*s))\n",
    "    return delta\n",
    "\n",
    "def tau(trg_phrase,src_phrase,phrase_counts):\n",
    "    if src_phrase and trg_phrase:\n",
    "        tau_value = (phrase_counts[(trg_phrase,src_phrase)] + \\\n",
    "                     (alpha * ((p_f(src_phrase,n_f) * \\\n",
    "                              calc_P_WA_trg_src(sent_ind, trg_phrase, sline)) * \\\n",
    "                             (p_e(trg_phrase,n_e) * \\\n",
    "                             calc_P_WA_src_trg(sent_ind, src_phrase, tline))))) / \\\n",
    "        (len(phrase_counts) + alpha)\n",
    "    else:\n",
    "        tau_value = 0\n",
    "    return tau_value\n",
    "\n",
    "def theta_N(trg_phrase,src_phrase):\n",
    "    if trg_phrase and src_phrase:\n",
    "        theta_N_value = 0\n",
    "    elif trg_phrase and not src_phrase:\n",
    "        theta_N_value = 0.5*p_e(trg_phrase,n_e)\n",
    "    elif src_phrase and not trg_phrase:\n",
    "        theta_N_value = 0.5*p_f(src_phrase,n_f)\n",
    "    return theta_N_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(src_phrase,trg_phrase,phrase_counts,tline,sline,sent_ind):\n",
    "    \"\"\"\n",
    "    This calculates the objective function's value.\n",
    "    Watch out! Need to be able to modify to accommodate Markov blanket...\n",
    "    \"\"\"\n",
    "    if src_phrase and trg_phrase:\n",
    "        obj_value = (1-p_S)*(1-p_phi)*tau(trg_phrase,src_phrase,phrase_counts)*delta(trg_phrase,src_phrase,tline,sline)\n",
    "    else:\n",
    "        obj_value = (1-p_S)*p_phi*theta_N(trg_phrase,src_phrase)\n",
    "        \n",
    "    return obj_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def model(L,):\n",
    "#    \"\"\"\n",
    "#    INCOMPLETE! Need to bring in phrases as input, and these\n",
    "#    will come from knowing sent_ind, sline, tline.\n",
    "#    L is the number of phrases in a sentence.\n",
    "#    \"\"\"\n",
    "#    for i, phrase in enumerate(phrase_pairs):\n",
    "#        delta(trg_phrase,src_phrase,tline,sline)\n",
    "#    first_term = p_S*((1-p_S)^(L-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.choice(5,1,p=[0,0.5,0.5,0,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SWAP(src_ph_1,trg_ph_1,src_ph_2,trg_ph_2,phrase_counts,phrase_table):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize scorekeeping dict\n",
    "    pair_score = dict()\n",
    "    # Set up configurations as tuples. Names of variables will be keys in dict.\n",
    "    ident = (src_ph_1,trg_ph_1,src_ph_2,trg_ph_2)\n",
    "    swapped = (src_ph_2,trg_ph_1,src_ph_1,trg_ph_2)\n",
    "    # First, modifying counts to take into account the Markov blanket.\n",
    "    phrase_counts[(src_ph_1,trg_ph_1)] -= 1\n",
    "    phrase_counts[(src_ph_2,trg_ph_2)] -= 1\n",
    "    # Do I need to decrement the counts for the swapped configuration?\n",
    "    # Second, calculate objective scores.\n",
    "    #     First, swapped configuration: increment phrase counts for it, \n",
    "    #     then decrement them to restore to Markov blanket\n",
    "    phrase_counts[(src_ph_2,trg_ph_1)] += 1\n",
    "    phrase_counts[(src_ph_1,trg_ph_2)] += 1\n",
    "    pair_score[\"swapped\"] = (objective(src_ph_2,trg_ph_1,phrase_counts,tline,sline,sent_ind),\n",
    "                             objective(src_ph_1,trg_ph_2,phrase_counts,tline,sline,sent_ind))\n",
    "    phrase_counts[(src_ph_2,trg_ph_1)] -= 1\n",
    "    phrase_counts[(src_ph_1,trg_ph_2)] -= 1\n",
    "    #     Second, identify configuration: increment phrase_counts for identity, \n",
    "    #     then decrement them to restore to Markov blanket\n",
    "    phrase_counts[(src_ph_1,trg_ph_1)] += 1\n",
    "    phrase_counts[(src_ph_2,trg_ph_2)] += 1\n",
    "    pair_score[\"ident\"] = (objective(src_ph_1,trg_ph_1,phrase_counts,tline,sline,sent_ind),\n",
    "                           objective(src_ph_2,trg_ph_2,phrase_counts,tline,sline,sent_ind))\n",
    "    phrase_counts[(src_ph_1,trg_ph_1)] -= 1\n",
    "    phrase_counts[(src_ph_2,trg_ph_2)] -= 1\n",
    "    # calculate output configuration probabilities\n",
    "    ident_prob = (pair_score[\"ident\"][0]*pair_score[\"ident\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "    swapped_prob = (pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "    # choose one configuration probabilistically\n",
    "    out_conf = np.random.choice([ident,swapped],p=[ident_prob,swapped_prob])[0]\n",
    "    # update phrase_counts, phrase_table depending on out_conf, which is a tuple of (f,e,f,e) phrases\n",
    "    phrase_counts[(out_conf[1],out_conf[0])] += 1\n",
    "    phrase_counts[(out_conf[3],out_conf[2])] += 1\n",
    "    # update phrase_table\n",
    "    pt = phrase_table[sent_id]\n",
    "    pt.remove((src_ph_1,trg_ph_1))\n",
    "    pt.remove((src_ph_2,trg_ph_2))\n",
    "    pt.add((out_conf[0],out_conf[1]))\n",
    "    pt.add((out_conf[2],out_conf[3]))\n",
    "    \n",
    "    return (phrase_counts, phrase_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-7f97f8903d34>, line 199)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-7f97f8903d34>\"\u001b[0;36m, line \u001b[0;32m199\u001b[0m\n\u001b[0;31m    elif if src_ph_1 == None and trg_ph_1 == None:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def collapsed_gibbs_aligner(srctext_file,trgtext_file,phrase_table,phrase_counts):\n",
    "    \"\"\"\n",
    "    This function assumes that relevant hyperparameters are set as \n",
    "    global variables with the appropriate names. This can be fixed\n",
    "    if necessary (or preferred)!\n",
    "    Will want to repeatedly call this for N iterations.\n",
    "    \"\"\"\n",
    "    # Supporting functions:\n",
    "    def p_f(src_phrase,n_f):\n",
    "        return (p_s*(1-p_s)**(len(src_phrase)))*((1/n_f)**(len(src_phrase)))\n",
    "    \n",
    "    def p_e(trg_phrase,n_e):\n",
    "        return (p_s*(1-p_s)**(len(trg_phrase)))*((1/n_e)**(len(trg_phrase)))\n",
    "    \n",
    "    def calc_P_WA_trg_src(sent_ind, trg_phrase, sline):\n",
    "        \"\"\"\n",
    "        Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "        if looking for P_WA(e|f), as in this function, trg_phrase is [(0,\"Michael\"),(1,\"assumes\")]\n",
    "        and sline is the source sentence in the form of a list: [\"Michael\", \"geht\", \"davon\", \"aus\", ...]\n",
    "        Should use dynamic programming to store calculations of phrase probabilities\n",
    "        based on IBM Model 1 word alignment probabilities.\n",
    "        \"\"\"\n",
    "        for i, pair in enumerate(trg_phrase):\n",
    "            # trg_phrase = ((0, 'This'), (1, 'is'))\n",
    "            # i = 0, pair = (0, 'This')\n",
    "            # i = 1, pair = (1, 'is')\n",
    "            if i in trg_align_dict[sent_ind][pair[0]]:\n",
    "                for word_ind in trg_align_dict[sent_ind][pair[0]]:\n",
    "                    if i == 0:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if sline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value = P_WA[pair[1]][sline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value = lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value = lowest_prob\n",
    "                    else:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if sline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value *= P_WA[pair[1]][sline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value *= lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value *= lowest_prob\n",
    "            else:\n",
    "                P_WA_value = lowest_prob\n",
    "        return P_WA_value\n",
    "    \n",
    "    def calc_P_WA_src_trg(sent_ind, src_phrase, tline):\n",
    "        \"\"\"\n",
    "        Expects both phrase arguments to be a list of 2-tuples, e.g.\n",
    "        if looking for P_WA(f|e), as in this function, src_phrase is [(0,\"Michael\"),(1,\"geht\"),(2,\"davon\"),(3,\"aus\")]\n",
    "        and tline is the target sentence in the form of a list: [\"Michael\", \"assumes\", ...]\n",
    "        Should use dynamic programming to store calculations of phrase probabilities\n",
    "        based on IBM Model 1 word alignment probabilities. \n",
    "        \"\"\"\n",
    "        for i, pair in enumerate(src_phrase):\n",
    "            if i in src_align_dict[sent_ind][pair[0]]:\n",
    "                for word_ind in src_align_dict[sent_ind][pair[0]]:\n",
    "                    if i == 0:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if tline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value = P_WA[pair[1]][tline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value = lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value = lowest_prob\n",
    "                    else:\n",
    "                        if pair[1] in P_WA:\n",
    "                            if tline[word_ind] in P_WA[pair[1]]:\n",
    "                                P_WA_value *= P_WA[pair[1]][tline[word_ind]]\n",
    "                            else:\n",
    "                                P_WA_value *= lowest_prob\n",
    "                        else:\n",
    "                            P_WA_value *= lowest_prob\n",
    "            else:\n",
    "                P_WA_value = lowest_prob\n",
    "        return P_WA_value\n",
    "    \n",
    "    def delta(trg_phrase,src_phrase,tline,sline):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        s = float(len(tline))/float(len(sline))\n",
    "        delta = b**abs(trg_phrase[0][0]-(src_phrase[0][0]*s))\n",
    "        return delta\n",
    "    \n",
    "    def tau(trg_phrase,src_phrase,phrase_counts):\n",
    "        tau_value = (phrase_counts[(trg_phrase,src_phrase)] + \\\n",
    "                     (alpha * ((p_f(src_phrase,n_f) * \\\n",
    "                              calc_P_WA_trg_src(sent_ind, trg_phrase, sline)) * \\\n",
    "                             (p_e(trg_phrase,n_e) * \\\n",
    "                             calc_P_WA_src_trg(sent_ind, src_phrase, tline))))) / \\\n",
    "        (len(phrase_counts) + alpha)\n",
    "        return tau_value\n",
    "    \n",
    "    def theta_N(trg_phrase,src_phrase):\n",
    "        if trg_phrase and not src_phrase:\n",
    "            theta_N_value = 0.5*p_e(trg_phrase,n_e)\n",
    "        elif src_phrase and not trg_phrase:\n",
    "            theta_N_value = 0.5*p_f(src_phrase,n_f)\n",
    "        return theta_N_value\n",
    "    \n",
    "    def objective(trg_phrase,src_phrase,phrase_counts,tline,sline,sent_ind):\n",
    "        \"\"\"\n",
    "        This calculates the potential function's value.\n",
    "        Watch out! Need to be able to modify to accommodate Markov blanket...\n",
    "        \"\"\"\n",
    "        if src_phrase and trg_phrase:\n",
    "            obj_value = (1-p_S)*(1-p_phi)*tau(trg_phrase,src_phrase,phrase_counts)*delta(trg_phrase,src_phrase,tline,sline)\n",
    "        else:\n",
    "            obj_value = (1-p_S)*p_phi*theta_N(trg_phrase,src_phrase)\n",
    "\n",
    "        return obj_value\n",
    "    \n",
    "    def create_phrase_map(sline,tline,full_phrases):\n",
    "        \"\"\"\n",
    "        full_phrases is the value of a key in the dictionary phrase_table, e.g. phrase_tables[0]\n",
    "        to get the full_phrases of the first sentence.\n",
    "        \"\"\"\n",
    "        src_phrase_map = {}\n",
    "        trg_phrase_map = {}\n",
    "        for phrase_ind, pair in enumerate(full_phrases):\n",
    "            trg = pair[0]\n",
    "            src = pair[1]\n",
    "            if trg:\n",
    "                for entry in trg:\n",
    "                    trg_phrase_map[entry[0]] = trg\n",
    "            if src:\n",
    "                for entry in src:\n",
    "                    src_phrase_map[entry[0]] = src\n",
    "        for src_ind, word in enumerate(sline):\n",
    "            if src_ind not in src_phrase_map:\n",
    "                src_phrase_map[src_ind] = None\n",
    "        for trg_ind, word in enumerate(tline):\n",
    "            if trg_ind not in trg_phrase_map:\n",
    "                trg_phrase_map[trg_ind] = None\n",
    "        return (src_phrase_map, trg_phrase_map)\n",
    "    \n",
    "    def SWAP(src_ph_1,trg_ph_1,src_ph_2,trg_ph_2,phrase_counts,phrase_table):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize scorekeeping dict\n",
    "        pair_score = dict()\n",
    "        # Set up configurations as tuples. Names of variables will be keys in dict.\n",
    "        ident = (src_ph_1,trg_ph_1,src_ph_2,trg_ph_2)\n",
    "        swapped = (src_ph_2,trg_ph_1,src_ph_1,trg_ph_2)\n",
    "        out_conf_dict = {\"ident\":ident,\"swapped\":swapped}\n",
    "        # First, modifying counts to take into account the Markov blanket.\n",
    "        phrase_counts[(src_ph_1,trg_ph_1)] -= 1\n",
    "        phrase_counts[(src_ph_2,trg_ph_2)] -= 1\n",
    "        # Do I need to decrement the counts for the swapped configuration?\n",
    "        # Second, calculate objective scores.\n",
    "        #     First, swapped configuration: increment phrase counts for it, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        phrase_counts[(src_ph_2,trg_ph_1)] += 1\n",
    "        phrase_counts[(src_ph_1,trg_ph_2)] += 1\n",
    "        pair_score[\"swapped\"] = (objective(trg_ph_1,src_ph_2,phrase_counts,tline,sline,sent_ind),\n",
    "                                 objective(trg_ph_2,src_ph_1,phrase_counts,tline,sline,sent_ind))\n",
    "        phrase_counts[(src_ph_2,trg_ph_1)] -= 1\n",
    "        phrase_counts[(src_ph_1,trg_ph_2)] -= 1\n",
    "        #     Second, identify configuration: increment phrase_counts for identity, \n",
    "        #     then decrement them to restore to Markov blanket\n",
    "        phrase_counts[(src_ph_1,trg_ph_1)] += 1\n",
    "        phrase_counts[(src_ph_2,trg_ph_2)] += 1\n",
    "        pair_score[\"ident\"] = (objective(trg_ph_1,src_ph_1,phrase_counts,tline,sline,sent_ind),\n",
    "                               objective(trg_ph_2,src_ph_2,phrase_counts,tline,sline,sent_ind))\n",
    "        phrase_counts[(src_ph_1,trg_ph_1)] -= 1\n",
    "        phrase_counts[(src_ph_2,trg_ph_2)] -= 1\n",
    "        # calculate output configuration probabilities\n",
    "        ident_prob = (pair_score[\"ident\"][0]*pair_score[\"ident\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        swapped_prob = (pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1])/((pair_score[\"ident\"][0]*pair_score[\"ident\"][1])+(pair_score[\"swapped\"][0]*pair_score[\"swapped\"][1]))\n",
    "        # choose one configuration probabilistically\n",
    "        out_conf_decision = np.random.choice([\"ident\",\"swapped\"],1,p=[ident_prob,swapped_prob])[0]\n",
    "        out_conf = out_conf_dict[out_conf_decision]\n",
    "        # update phrase_counts, phrase_table depending on out_conf, which is a tuple of (f,e,f,e) phrases\n",
    "        phrase_counts[(out_conf[1],out_conf[0])] += 1\n",
    "        phrase_counts[(out_conf[3],out_conf[2])] += 1\n",
    "        # update phrase_table\n",
    "        pt = phrase_table[sent_ind]\n",
    "#        for i in pt:\n",
    "#            print(i)\n",
    "        pt.remove((trg_ph_1,src_ph_1))\n",
    "        pt.remove((trg_ph_2,src_ph_2))\n",
    "        pt.add((out_conf[1],out_conf[0]))\n",
    "        pt.add((out_conf[3],out_conf[2]))\n",
    "        phrase_table[sent_ind] = pt\n",
    "        \n",
    "        return (phrase_counts, phrase_table)\n",
    "    \n",
    "    def FLIP():\n",
    "        pass\n",
    "    \n",
    "    def TOGGLE(src_ph,trg_ph,phrase_counts,phrase_table):\n",
    "        pair_score = dict()\n",
    "        prob_score[] = dict()\n",
    "        if src_ph_1 != None and trg_ph_1 != None:\n",
    "            delinked = ((trg_ph_1,None),(None,src_ph_1))\n",
    "            ident = (trg_ph_1,src_ph_1)\n",
    "            # Take out phrase_count for this pair to accommodate markov blanket\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            # see what effect delinking (i.e. \"unaligning\") them has\n",
    "            pair_score[\"delinked\"] = objective(trg_ph_1,None,phrase_counts,tline,sline,sent_ind)*objective(None,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # Augment phrase_counts to capture \"new\" pairing (which was originally there, in this case)\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) += 1\n",
    "            pair_score[\"ident\"] = objective(trg_ph_1,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # Restore Markov blanket, and await decision as to shape this should take below.\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            prob_score[\"ident\"] = pair_score[\"ident\"]/(pair_score[\"ident\"]+pair_score[\"delinked\"])\n",
    "            prob_score[\"delinked\"] = pair_score[\"delinked\"]/(pair_score[\"ident\"]+pair_score[\"delinked\"])\n",
    "            out_conf_decision = np.random.choice([\"ident\",\"delinked\"],1,p=[prob_score[\"ident\"],prob_score[\"delinked\"]])\n",
    "            if out_conf_decision == \"ident\":\n",
    "                phrase_counts((trg_ph_1,src_ph_1)) += 1\n",
    "            elif out_conf_decision == \"delinked\":\n",
    "                phrase_table[sent_ind].remove((trg_ph_1,src_ph_1))\n",
    "        elif src_ph_1 == None and trg_ph_1 == None:\n",
    "            # create Markov blanket\n",
    "            phrase_counts((trg_ph_1,src_ph_1)) -= 1\n",
    "            # see what effect aligning them has\n",
    "            aligned = (trg_ph_1,src_ph_1)\n",
    "            pair_score[\"delinked\"] = objective(trg_ph_1,src_ph_1,phrase_counts,tline,sline,sent_ind)\n",
    "            # START WORKING HERE ON 1/13\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return (phrase_counts, phrase_table)\n",
    "    \n",
    "    \n",
    "    \n",
    "    prob_table = {}   # keys are sent_ind, values are final probabilities.\n",
    "    with open(srctext_file,\"r\") as srctext_f, \\\n",
    "    open(trgtext_file,\"r\") as trgtext_f:\n",
    "        for sent_ind, (src_line, trg_line) in enumerate(zip(srctext_f,trgtext_f)):\n",
    "            sline = src_line.strip(\"\\n\\r\").split()\n",
    "            tline = trg_line.strip(\"\\n\\r\").split()\n",
    "            # Need to loop through indices of both source and target sentences\n",
    "            # since their lengths are likely not equal.\n",
    "#            initial_probability = 0   # PLACEHOLDER - fill with model calculation. Not clear this is needed...\n",
    "            # Need to write a function for here that maps each index in the english sentence \n",
    "            # and in the foreign sentence to the phrase in which it is included.\n",
    "            (src_phrase_map, trg_phrase_map) = create_phrase_map(sline,tline,phrase_table[sent_ind])\n",
    "            # SPEED ISSUE would probably be here because of nested for-loop.\n",
    "            src_prev_phrase = \"\"\n",
    "            trg_prev_phrase = \"\"\n",
    "            for src_ind, src_word in enumerate(sline):\n",
    "                for trg_ind, trg_word in enumerate(tline):\n",
    "                    # Apply SWAP, if applicable - could remove some of these for-loops with a dictionary \n",
    "                    # of phrase alignments.\n",
    "                    # CHECK TO ENSURE THAT WE'RE NOT MISSING ANY PHRASES IN EITHER LANGUAGE!\n",
    "                    src_ph_1 = src_phrase_map[src_ind]\n",
    "                    for pair_1 in phrase_table[sent_ind]:\n",
    "                        if pair_1[1] == src_ph_1:\n",
    "                            trg_ph_1 = pair_1[0]\n",
    "                        else:\n",
    "                            trg_ph_1 = None\n",
    "                    # We do this if-loop to make sure that we're not unnecessarily recalculating\n",
    "                    # SWAP when we're changing sentence index, but not actually phrase boundaries\n",
    "                    if src_phrase_map[src_ind] != src_prev_phrase and trg_phrase_map[trg_ind] != trg_prev_phrase:\n",
    "                        for ind in range(src_ind,len(sline)):\n",
    "                            if src_phrase_map[ind] != src_ph_1:\n",
    "                                src_ph_2 = src_phrase_map[ind]\n",
    "                                for pair_2 in phrase_table[sent_ind]:\n",
    "                                    if pair_2[1] == src_ph_2:\n",
    "                                        trg_ph_2 = pair_2[0]\n",
    "                                break\n",
    "                        if src_ph_2 and trg_ph_2:\n",
    "                            # Actual call to SWAP\n",
    "                            phrase_counts, phrase_table = SWAP(src_ph_1,trg_ph_1,src_ph_2,trg_ph_2,phrase_counts,phrase_table)\n",
    "                            src_prev_phrase = src_ph_1\n",
    "                            trg_prev_phrase = trg_ph_1\n",
    "                            src_ph_2 = None\n",
    "                            trg_ph_2 = None\n",
    "                    # Apply TOGGLE, if applicable\n",
    "                    #TOGGLE()\n",
    "                    # Apply FLIP, if applicable\n",
    "                    \n",
    "                    \n",
    "                    # Apply FLIP TWO, if applicable\n",
    "                    \n",
    "                    # Apply MOVE, if applicable\n",
    "                    \n",
    "#            final_probability = 0   # PLACEHOLDER - fill with model calculation.\n",
    "    return phrase_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5341612078038177 0.46583879219618224 1.0\n",
      "0.43199502836797 0.5680049716320299 0.9999999999999999\n",
      "0.5299019725920529 0.4700980274079471 1.0\n",
      "0.5595908179171032 0.4404091820828968 1.0\n",
      "0.5085527935361984 0.49144720646380163 1.0\n",
      "0.679770921747692 0.32022907825230806 1.0\n",
      "1.0 2.731865482750061e-19 1.0\n",
      "1.0 1.1585706763125876e-23 1.0\n",
      "0.6053198708907699 0.39468012910923006 1.0\n",
      "0.5469066148801611 0.4530933851198389 1.0\n",
      "0.5929947898610891 0.407005210138911 1.0\n",
      "0.5837141848077462 0.41628581519225377 1.0\n",
      "0.5421546145467243 0.45784538545327574 1.0\n",
      "0.45784538545327574 0.5421546145467242 1.0\n",
      "0.4578453854532757 0.5421546145467243 1.0\n",
      "0.4578453854532758 0.5421546145467242 1.0\n",
      "1.0 3.186585618285712e-19 1.0\n",
      "1.0 1.2661915423640027e-32 1.0\n",
      "1.0 3.230072617011702e-28 1.0\n",
      "0.5837141848077463 0.41628581519225377 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Data Structures\n",
    "    # Hyperparameters - safe to set as global variables since no function should change these on a given run.\n",
    "    p_S = 0.1    # p_$\n",
    "    b = 0.85\n",
    "    p_phi = 10e-10\n",
    "    alpha = 100\n",
    "    p_s = 0.8    # p_s -- note difference from p_S = p_$ = 0.1\n",
    "    \n",
    "    # First, initialize the model.\n",
    "    src_vocab_file = \"../data/example/two_line/f-srctext-spa.vcb\"\n",
    "    trg_vocab_file = \"../data/example/two_line/e-trgtext-eng.vcb\"\n",
    "    src_trg_alignment = \"../data/example/two_line/f_e_word_alignment_probs.txt\"\n",
    "    trg_src_alignment = \"../data/example/two_line/e_f_word_alignment_probs.txt\"\n",
    "    srctext_file = \"../data/example/two_line/f-srctext-spa.txt\"\n",
    "    trgtext_file = \"../data/example/two_line/e-trgtext-eng.txt\"\n",
    "    alignments_file = \"../data/example/two_line/berkeley_alignments_spa_eng.txt\"\n",
    "    \n",
    "    P_WA, lowest_prob = model_one_processing(src_vocab_file,trg_vocab_file,src_trg_alignment,trg_src_alignment)\n",
    "    lowest_prob *= 0.1\n",
    "    phrase_counts, phrase_table, src_align_dict, trg_align_dict, unigram_e_len, unigram_f_len, n_e, n_f = initialize_phrases_from_text(srctext_file,trgtext_file,alignments_file)\n",
    "    orig_phrase_table = copy.deepcopy(phrase_table)    # DEBUGGING\n",
    "    phrase_table = collapsed_gibbs_aligner(srctext_file,trgtext_file,phrase_table,phrase_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
